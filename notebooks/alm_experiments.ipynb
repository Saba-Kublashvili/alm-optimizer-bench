{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e040d3ce-aefc-4ad9-bd1b-71ebb314bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install torch torchvision torchaudio tqdm pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5198848-e568-4bf0-81d6-3b2f6e2876bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def bootstrap_ci(x: List[float], seed: int = 0, n: int = 5000, alpha: float = 0.05):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    means = []\n",
    "    for _ in range(n):\n",
    "        samp = rng.choice(x, size=len(x), replace=True)\n",
    "        means.append(float(np.mean(samp)))\n",
    "    means = np.array(means)\n",
    "    lo = float(np.quantile(means, alpha/2))\n",
    "    hi = float(np.quantile(means, 1 - alpha/2))\n",
    "    return float(np.mean(x)), lo, hi\n",
    "\n",
    "def paired_permutation_test(x: List[float], y: List[float], seed: int = 0, n: int = 20000):\n",
    "    \"\"\"\n",
    "    H0: mean(x-y) = 0 under random sign flips of paired differences.\n",
    "    Returns two-sided p-value.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "    d = x - y\n",
    "    obs = abs(float(np.mean(d)))\n",
    "    count = 0\n",
    "    for _ in range(n):\n",
    "        signs = rng.choice([-1.0, 1.0], size=len(d))\n",
    "        stat = abs(float(np.mean(signs * d)))\n",
    "        if stat >= obs:\n",
    "            count += 1\n",
    "    return (count + 1) / (n + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c603886d-dec5-4cfc-97c7-5b42e97500f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = \"cifar10\"          # \"cifar10\" or \"cifar100\"\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    val_frac: float = 0.1\n",
    "    data_root: str = \"./data\"\n",
    "\n",
    "def make_loaders(data_cfg: DataConfig, seed: int):\n",
    "    if data_cfg.dataset.lower() == \"cifar10\":\n",
    "        num_classes = 10\n",
    "        ds_cls = torchvision.datasets.CIFAR10\n",
    "        mean = (0.4914, 0.4822, 0.4465)\n",
    "        std  = (0.2023, 0.1994, 0.2010)\n",
    "    elif data_cfg.dataset.lower() == \"cifar100\":\n",
    "        num_classes = 100\n",
    "        ds_cls = torchvision.datasets.CIFAR100\n",
    "        mean = (0.5071, 0.4867, 0.4408)\n",
    "        std  = (0.2675, 0.2565, 0.2761)\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be cifar10 or cifar100\")\n",
    "\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    full_train = ds_cls(root=data_cfg.data_root, train=True, download=True, transform=train_tf)\n",
    "    test_ds    = ds_cls(root=data_cfg.data_root, train=False, download=True, transform=test_tf)\n",
    "\n",
    "    n = len(full_train)\n",
    "    n_val = int(round(data_cfg.val_frac * n))\n",
    "    n_train = n - n_val\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "\n",
    "    train_ds, val_ds = random_split(full_train, [n_train, n_val], generator=gen)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=data_cfg.batch_size, shuffle=True,\n",
    "                              num_workers=data_cfg.num_workers, pin_memory=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False,\n",
    "                              num_workers=data_cfg.num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False,\n",
    "                              num_workers=data_cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85abe991-101a-416c-835b-baff04fb844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18, resnet34\n",
    "\n",
    "def make_resnet_cifar(depth: str, num_classes: int):\n",
    "    depth = depth.lower()\n",
    "    if depth == \"resnet18\":\n",
    "        m = resnet18(num_classes=num_classes)\n",
    "    elif depth == \"resnet34\":\n",
    "        m = resnet34(num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"depth must be resnet18 or resnet34\")\n",
    "\n",
    "    # CIFAR stem: 3x3 conv, stride 1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bf1841-28aa-459a-99d3-23922ae1d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _power_iter_sigma(W: torch.Tensor, u: torch.Tensor = None, n_iter: int = 1, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Returns (sigma, u_new).\n",
    "    W is treated as a matrix.\n",
    "    \"\"\"\n",
    "    if W.ndim > 2:\n",
    "        Wm = W.flatten(1)   # (out, in*k*k) for conv weights\n",
    "    else:\n",
    "        Wm = W\n",
    "\n",
    "    out_dim = Wm.shape[0]\n",
    "    if u is None or u.numel() != out_dim:\n",
    "        u = torch.randn(out_dim, device=W.device, dtype=torch.float32)\n",
    "    u = u.to(dtype=torch.float32)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        v = torch.mv(Wm.t(), u)\n",
    "        v = v / (v.norm() + eps)\n",
    "        u = torch.mv(Wm, v)\n",
    "        u = u / (u.norm() + eps)\n",
    "\n",
    "    sigma = torch.dot(u, torch.mv(Wm, v)).abs()\n",
    "    sigma = torch.clamp(sigma, min=eps)\n",
    "    return sigma, u\n",
    "\n",
    "@torch.no_grad()\n",
    "def resnet_logK(model: nn.Module, state: Dict[str, Any], power_iter: int = 1):\n",
    "    \"\"\"\n",
    "    Surrogate log Lipschitz upper bound for CIFAR-ResNet:\n",
    "      logK = log||stem|| + sum_blocks log(1 + K_res(block)) + log||fc||\n",
    "    with K_res(block) ≈ product of spectral norms along residual path.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    logK = torch.zeros((), device=device, dtype=torch.float32)\n",
    "\n",
    "    # stem conv1\n",
    "    W = model.conv1.weight\n",
    "    key = (\"stem\", id(W))\n",
    "    sigma, u = _power_iter_sigma(W, state.get(key), n_iter=power_iter)\n",
    "    state[key] = u\n",
    "    logK = logK + torch.log(sigma)\n",
    "\n",
    "    # helper: conv sigma\n",
    "    def conv_sigma(conv: nn.Conv2d, tag: str):\n",
    "        W = conv.weight\n",
    "        key = (tag, id(W))\n",
    "        sigma, u = _power_iter_sigma(W, state.get(key), n_iter=power_iter)\n",
    "        state[key] = u\n",
    "        return sigma\n",
    "\n",
    "    # each residual block: log(1 + prod(sigmas in residual branch))\n",
    "    for li, layer in enumerate([model.layer1, model.layer2, model.layer3, model.layer4], start=1):\n",
    "        for bi, block in enumerate(layer):\n",
    "            # BasicBlock (2 convs) or Bottleneck (3 convs)\n",
    "            sigmas = []\n",
    "            sigmas.append(conv_sigma(block.conv1, f\"l{li}b{bi}.conv1\"))\n",
    "            sigmas.append(conv_sigma(block.conv2, f\"l{li}b{bi}.conv2\"))\n",
    "            if hasattr(block, \"conv3\"):  # Bottleneck\n",
    "                sigmas.append(conv_sigma(block.conv3, f\"l{li}b{bi}.conv3\"))\n",
    "            # downsample conv, if present (affects skip path, but for safety we include it into residual path multiplier)\n",
    "            if block.downsample is not None:\n",
    "                # typically downsample[0] is conv\n",
    "                ds0 = block.downsample[0]\n",
    "                if isinstance(ds0, nn.Conv2d):\n",
    "                    sigmas.append(conv_sigma(ds0, f\"l{li}b{bi}.down\"))\n",
    "\n",
    "            # logK_res = sum log sigma_i\n",
    "            logK_res = torch.zeros((), device=device, dtype=torch.float32)\n",
    "            for s in sigmas:\n",
    "                logK_res = logK_res + torch.log(torch.clamp(s, min=1e-12))\n",
    "\n",
    "            # logK_block = log(1 + exp(logK_res)) = softplus(logK_res)\n",
    "            logK = logK + F.softplus(logK_res)\n",
    "\n",
    "    # fc\n",
    "    W = model.fc.weight\n",
    "    key = (\"fc\", id(W))\n",
    "    sigma, u = _power_iter_sigma(W, state.get(key), n_iter=power_iter)\n",
    "    state[key] = u\n",
    "    logK = logK + torch.log(sigma)\n",
    "\n",
    "    return logK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56ad69a-68b4-47fd-909e-948143d61b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def alm_update_state(\n",
    "    logK_value: torch.Tensor,          # scalar tensor\n",
    "    target_logK: float,\n",
    "    margin: float,\n",
    "    st: Dict[str, Any],\n",
    "    *,\n",
    "    ema_beta: float = 0.95,\n",
    "    dual_lr: float = 0.1,              # η_λ\n",
    "    rho_growth: float = 1.3,           # γ\n",
    "    rho_min: float = 1e-4,\n",
    "    rho_max: float = 10.0,\n",
    "    lam_max: float = 10.0,\n",
    "    tol_hi: float = 5e-3,\n",
    "    tol_lo: float = 5e-4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Constraint: g = logK - (target_logK + margin) <= 0.\n",
    "    Uses EMA-smoothed g, hinge g_+ = max(0,g), and dual update allowing λ to decrease.\n",
    "    \"\"\"\n",
    "    g = (logK_value - (target_logK + margin)).float()\n",
    "\n",
    "    g_ema = st.get(\"g_ema\", torch.zeros((), device=g.device, dtype=g.dtype))\n",
    "    g_ema = ema_beta * g_ema + (1.0 - ema_beta) * g\n",
    "    st[\"g_ema\"] = g_ema\n",
    "\n",
    "    g_pos = torch.relu(g_ema)\n",
    "    st[\"g_pos\"] = g_pos\n",
    "\n",
    "    lam = float(st.get(\"lam\", 0.0))\n",
    "    rho = float(st.get(\"rho\", 0.5))\n",
    "\n",
    "    lam = lam + dual_lr * rho * float(g_ema.item())\n",
    "    lam = max(0.0, min(lam, lam_max))\n",
    "\n",
    "    # ρ adaptation based on violation only\n",
    "    if float(g_pos.item()) > tol_hi:\n",
    "        rho = min(rho_max, rho * rho_growth)\n",
    "    elif float(g_pos.item()) < tol_lo:\n",
    "        rho = max(rho_min, rho / rho_growth)\n",
    "\n",
    "    st[\"lam\"] = lam\n",
    "    st[\"rho\"] = rho\n",
    "\n",
    "    return float(g.item()), float(g_ema.item()), float(g_pos.item()), lam, rho\n",
    "\n",
    "def alm_penalty(g_pos: torch.Tensor, lam: float, rho: float):\n",
    "    return (lam * g_pos) + (0.5 * rho * (g_pos ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95a0295-80aa-4fd1-a161-969160ee5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def alm_update_state(\n",
    "    logK_value: torch.Tensor,          # scalar tensor\n",
    "    target_logK: float,\n",
    "    margin: float,\n",
    "    st: Dict[str, Any],\n",
    "    *,\n",
    "    ema_beta: float = 0.95,\n",
    "    dual_lr: float = 0.1,              # η_λ\n",
    "    rho_growth: float = 1.3,           # γ\n",
    "    rho_min: float = 1e-4,\n",
    "    rho_max: float = 10.0,\n",
    "    lam_max: float = 10.0,\n",
    "    tol_hi: float = 5e-3,\n",
    "    tol_lo: float = 5e-4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Constraint: g = logK - (target_logK + margin) <= 0.\n",
    "    Uses EMA-smoothed g, hinge g_+ = max(0,g), and dual update allowing λ to decrease.\n",
    "    \"\"\"\n",
    "    g = (logK_value - (target_logK + margin)).float()\n",
    "\n",
    "    g_ema = st.get(\"g_ema\", torch.zeros((), device=g.device, dtype=g.dtype))\n",
    "    g_ema = ema_beta * g_ema + (1.0 - ema_beta) * g\n",
    "    st[\"g_ema\"] = g_ema\n",
    "\n",
    "    g_pos = torch.relu(g_ema)\n",
    "    st[\"g_pos\"] = g_pos\n",
    "\n",
    "    lam = float(st.get(\"lam\", 0.0))\n",
    "    rho = float(st.get(\"rho\", 0.5))\n",
    "\n",
    "    lam = lam + dual_lr * rho * float(g_ema.item())\n",
    "    lam = max(0.0, min(lam, lam_max))\n",
    "\n",
    "    # ρ adaptation based on violation only\n",
    "    if float(g_pos.item()) > tol_hi:\n",
    "        rho = min(rho_max, rho * rho_growth)\n",
    "    elif float(g_pos.item()) < tol_lo:\n",
    "        rho = max(rho_min, rho / rho_growth)\n",
    "\n",
    "    st[\"lam\"] = lam\n",
    "    st[\"rho\"] = rho\n",
    "\n",
    "    return float(g.item()), float(g_ema.item()), float(g_pos.item()), lam, rho\n",
    "\n",
    "def alm_penalty(g_pos: torch.Tensor, lam: float, rho: float):\n",
    "    return (lam * g_pos) + (0.5 * rho * (g_pos ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc09c30-ed43-4c78-acb1-20999770e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    depth: str = \"resnet18\"\n",
    "    epochs: int = 60\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.02\n",
    "    amp: bool = True\n",
    "\n",
    "    # ALM switches\n",
    "    use_alm: bool = False\n",
    "    margin: float = 0.0\n",
    "    rho0: float = 0.5\n",
    "    dual_lr: float = 0.1\n",
    "    rho_growth: float = 1.3\n",
    "    rho_max: float = 10.0\n",
    "    lam_max: float = 10.0\n",
    "    ema_beta: float = 0.95\n",
    "    power_iter: int = 1\n",
    "\n",
    "    # target calibration: target_logK = logK_init + budget_delta\n",
    "    budget_delta: float = 0.5\n",
    "\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss_sum += float(ce(logits, y).item()) * y.size(0)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += int((pred == y).sum().item())\n",
    "            total += int(y.size(0))\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "def train_one_run(seed: int, data_cfg: DataConfig, cfg: TrainConfig, device: str = \"cuda\", log_every: int = 20):\n",
    "    seed_all(seed)\n",
    "\n",
    "    train_loader, val_loader, test_loader, num_classes = make_loaders(data_cfg, seed=seed)\n",
    "\n",
    "    model = make_resnet_cifar(cfg.depth, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(cfg.amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    # ALM state\n",
    "    spec_state: Dict[str, Any] = {}\n",
    "    alm_state: Dict[str, Any] = {\"lam\": 0.0, \"rho\": float(cfg.rho0)}\n",
    "\n",
    "    # Calibrate target logK once per run (weights only, no data)\n",
    "    with torch.no_grad():\n",
    "        lk0 = float(resnet_logK(model, spec_state, power_iter=cfg.power_iter).item())\n",
    "    target_logK = lk0 + float(cfg.budget_delta)\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_test_at_best_val = -1.0\n",
    "\n",
    "    hist_rows = []\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_sum = 0.0\n",
    "\n",
    "        # epoch aggregates for logging\n",
    "        last_logK = float(\"nan\")\n",
    "        last_g = float(\"nan\")\n",
    "        last_gema = float(\"nan\")\n",
    "        last_gpos = float(\"nan\")\n",
    "        last_lam = float(\"nan\")\n",
    "        last_rho = float(\"nan\")\n",
    "\n",
    "        for it, (x, y) in enumerate(train_loader, start=1):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=(cfg.amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                ce_loss = criterion(logits, y)\n",
    "\n",
    "                if cfg.use_alm:\n",
    "                    logK = resnet_logK(model, spec_state, power_iter=cfg.power_iter)\n",
    "                    g, g_ema, g_pos, lam, rho = alm_update_state(\n",
    "                        logK, target_logK=target_logK, margin=cfg.margin, st=alm_state,\n",
    "                        ema_beta=cfg.ema_beta, dual_lr=cfg.dual_lr,\n",
    "                        rho_growth=cfg.rho_growth, rho_max=cfg.rho_max, lam_max=cfg.lam_max\n",
    "                    )\n",
    "                    penalty = alm_penalty(alm_state[\"g_pos\"], lam=alm_state[\"lam\"], rho=alm_state[\"rho\"])\n",
    "                    loss = ce_loss + penalty\n",
    "\n",
    "                    last_logK = float(logK.item())\n",
    "                    last_g, last_gema, last_gpos, last_lam, last_rho = g, g_ema, g_pos, lam, rho\n",
    "                else:\n",
    "                    loss = ce_loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += int((pred == y).sum().item())\n",
    "            total += int(y.size(0))\n",
    "            loss_sum += float(ce_loss.item()) * y.size(0)\n",
    "\n",
    "        sched.step()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_ce = loss_sum / total\n",
    "\n",
    "        val_ce, val_acc = accuracy(model, val_loader, device)\n",
    "        test_ce, test_acc = accuracy(model, test_loader, device)\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_test_at_best_val = test_acc\n",
    "\n",
    "        row = {\n",
    "            \"seed\": seed,\n",
    "            \"method\": \"ALM\" if cfg.use_alm else \"AdamW\",\n",
    "            \"dataset\": data_cfg.dataset,\n",
    "            \"depth\": cfg.depth,\n",
    "            \"epoch\": ep,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_ce\": train_ce,\n",
    "            \"val_ce\": val_ce,\n",
    "            \"test_ce\": test_ce,\n",
    "            \"logK\": last_logK,\n",
    "            \"g\": last_g,\n",
    "            \"g_ema\": last_gema,\n",
    "            \"g_pos\": last_gpos,\n",
    "            \"lam\": last_lam,\n",
    "            \"rho\": last_rho,\n",
    "            \"target_logK\": target_logK\n",
    "        }\n",
    "        hist_rows.append(row)\n",
    "\n",
    "        if ep in (1, 10, 20, 30, 40, 50, 60) or (ep % log_every == 0) or (ep == cfg.epochs):\n",
    "            tag = \"ALM\" if cfg.use_alm else \"AdamW\"\n",
    "            if cfg.use_alm:\n",
    "                print(f\"[{tag}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} \"\n",
    "                      f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f} \"\n",
    "                      f\"logK={last_logK:.4f} g_ema={last_gema:.6f} lam={last_lam:.3f} rho={last_rho:.3f}\")\n",
    "            else:\n",
    "                print(f\"[{tag}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} \"\n",
    "                      f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f}\")\n",
    "\n",
    "    hist_df = pd.DataFrame(hist_rows)\n",
    "\n",
    "    summary = {\n",
    "        \"seed\": seed,\n",
    "        \"method\": \"ALM\" if cfg.use_alm else \"AdamW\",\n",
    "        \"dataset\": data_cfg.dataset,\n",
    "        \"depth\": cfg.depth,\n",
    "        \"epochs\": cfg.epochs,\n",
    "        \"lr\": cfg.lr,\n",
    "        \"wd\": cfg.weight_decay,\n",
    "        \"best_val_acc\": float(best_val),\n",
    "        \"test_at_best_val\": float(best_test_at_best_val),\n",
    "        \"final_test_acc\": float(hist_df[hist_df.epoch == cfg.epochs][\"test_acc\"].iloc[0]),\n",
    "        \"target_logK\": float(target_logK),\n",
    "        \"margin\": float(cfg.margin) if cfg.use_alm else float(\"nan\"),\n",
    "        \"rho0\": float(cfg.rho0) if cfg.use_alm else float(\"nan\"),\n",
    "        \"budget_delta\": float(cfg.budget_delta) if cfg.use_alm else float(\"nan\"),\n",
    "    }\n",
    "    return hist_df, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d455e38b-29bd-4af5-a834-529b5711a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "\n",
      "================================================================================\n",
      "ALM RUN seed= 1\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 97820468.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4811 val_acc=0.5632 test_acc=0.5758 logK=14.4279 g_ema=4.572793 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8778 val_acc=0.8384 test_acc=0.8531 logK=19.7745 g_ema=9.957969 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9428 val_acc=0.8790 test_acc=0.8822 logK=22.1051 g_ema=12.308013 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9752 val_acc=0.9026 test_acc=0.9023 logK=23.1397 g_ema=13.336095 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9919 val_acc=0.9094 test_acc=0.9096 logK=23.4439 g_ema=13.654722 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9984 val_acc=0.9210 test_acc=0.9175 logK=23.5013 g_ema=13.709080 lam=10.000 rho=10.000\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9992 val_acc=0.9216 test_acc=0.9199 logK=23.5025 g_ema=13.709332 lam=10.000 rho=10.000\n",
      "\n",
      "================================================================================\n",
      "ALM RUN seed= 2\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4852 val_acc=0.5672 test_acc=0.5891 logK=14.5643 g_ema=4.561891 lam=10.000 rho=10.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x790d87ef7600>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 6319, 6320, 6321) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/usr/lib/python3.11/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALM RUN seed=\u001b[39m\u001b[38;5;124m\"\u001b[39m, s)\n\u001b[0;32m---> 33\u001b[0m h, summ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malm_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m all_hist\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m     35\u001b[0m all_sum\u001b[38;5;241m.\u001b[39mappend(summ)\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mtrain_one_run\u001b[0;34m(seed, data_cfg, cfg, device, log_every)\u001b[0m\n\u001b[1;32m     78\u001b[0m last_lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m last_rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 6319, 6320, 6321) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "data_cfg = DataConfig(dataset=\"cifar10\", batch_size=128, num_workers=4, val_frac=0.1)\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "alm_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "\n",
    "    use_alm=True,\n",
    "    # IMPORTANT knobs:\n",
    "    budget_delta=0.5,   # looser than your earlier overly-tight target; raise if accuracy drops\n",
    "    margin=0.0,\n",
    "    rho0=0.5,\n",
    "    dual_lr=0.1,\n",
    "    rho_growth=1.3,\n",
    "    rho_max=10.0,\n",
    "    lam_max=10.0,\n",
    "    ema_beta=0.95,\n",
    "    power_iter=1\n",
    ")\n",
    "\n",
    "all_hist = []\n",
    "all_sum = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALM RUN seed=\", s)\n",
    "    h, summ = train_one_run(s, data_cfg, alm_cfg, device=device, log_every=20)\n",
    "    all_hist.append(h)\n",
    "    all_sum.append(summ)\n",
    "\n",
    "alm_summary = pd.DataFrame(all_sum)\n",
    "alm_history = pd.concat(all_hist, ignore_index=True)\n",
    "\n",
    "display(alm_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca39a66-d39a-4d9d-bdc6-a7f6ce7da063",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "    use_alm=False\n",
    ")\n",
    "\n",
    "all_sum_a = []\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AdamW RUN seed=\", s)\n",
    "    _, summ = train_one_run(s, data_cfg, adam_cfg, device=device, log_every=20)\n",
    "    all_sum_a.append(summ)\n",
    "\n",
    "adam_summary = pd.DataFrame(all_sum_a)\n",
    "display(adam_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03baee62-3a4c-43d9-b476-ac4f0d21d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_summaries(adam_df: pd.DataFrame, alm_df: pd.DataFrame):\n",
    "    # Align by seed\n",
    "    seeds_common = sorted(set(adam_df.seed.tolist()) & set(alm_df.seed.tolist()))\n",
    "    a = []\n",
    "    b = []\n",
    "    for s in seeds_common:\n",
    "        a.append(float(adam_df[adam_df.seed==s][\"test_at_best_val\"].iloc[0]))\n",
    "        b.append(float(alm_df[alm_df.seed==s][\"test_at_best_val\"].iloc[0]))\n",
    "\n",
    "    mean_a, lo_a, hi_a = bootstrap_ci(a, seed=0)\n",
    "    mean_b, lo_b, hi_b = bootstrap_ci(b, seed=1)\n",
    "    pval = paired_permutation_test(b, a, seed=2)\n",
    "\n",
    "    print(\"Paired metric: test_at_best_val (paired by identical seeds)\")\n",
    "    print(f\"AdamW mean={mean_a:.4f}  95%CI=[{lo_a:.4f},{hi_a:.4f}]\")\n",
    "    print(f\"ALM   mean={mean_b:.4f}  95%CI=[{lo_b:.4f},{hi_b:.4f}]\")\n",
    "    print(f\"Paired permutation p-value (ALM vs AdamW): {pval:.6f}\")\n",
    "\n",
    "# If you ran Cell 9:\n",
    "compare_summaries(adam_summary, alm_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe499a9-a889-411f-b1d0-593494f7354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip -q install numpy pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d55c55-f0cd-405c-be74-41503a70665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516a597f-7924-4a63-84b2-a4ff2baeb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = \"cifar10\"\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    val_frac: float = 0.1\n",
    "    split_seed: int = 123  # fixed so AdamW vs ALM are paired on same split\n",
    "\n",
    "def make_loaders(cfg: DataConfig):\n",
    "    if cfg.dataset.lower() == \"cifar10\":\n",
    "        num_classes = 10\n",
    "        ds_cls = torchvision.datasets.CIFAR10\n",
    "    elif cfg.dataset.lower() == \"cifar100\":\n",
    "        num_classes = 100\n",
    "        ds_cls = torchvision.datasets.CIFAR100\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be cifar10 or cifar100\")\n",
    "\n",
    "    # Standard CIFAR preprocessing\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "    tf_train = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    tf_test = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_full = ds_cls(root=\"./data\", train=True, download=True, transform=tf_train)\n",
    "    test_set   = ds_cls(root=\"./data\", train=False, download=True, transform=tf_test)\n",
    "\n",
    "    n = len(train_full)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.RandomState(cfg.split_seed)\n",
    "    rng.shuffle(idx)\n",
    "    n_val = int(cfg.val_frac * n)\n",
    "    val_idx = idx[:n_val].tolist()\n",
    "    tr_idx  = idx[n_val:].tolist()\n",
    "\n",
    "    # For validation, use test transforms (no aug)\n",
    "    val_set = ds_cls(root=\"./data\", train=True, download=False, transform=tf_test)\n",
    "    train_set = train_full\n",
    "\n",
    "    train_loader = DataLoader(Subset(train_set, tr_idx), batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(Subset(val_set, val_idx), batch_size=cfg.batch_size, shuffle=False,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "648c6f9c-79cf-44b9-911c-874d0ae45661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet_cifar(depth: str, num_classes: int):\n",
    "    depth = depth.lower()\n",
    "    if depth == \"resnet18\":\n",
    "        m = torchvision.models.resnet18(num_classes=num_classes)\n",
    "    elif depth == \"resnet34\":\n",
    "        m = torchvision.models.resnet34(num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"depth must be resnet18 or resnet34\")\n",
    "\n",
    "    # CIFAR stem: 3x3 conv, stride 1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec90256-8909-4dc2-9ad7-68cddfe03568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTracker:\n",
    "    \"\"\"\n",
    "    Tracks normalized spectral norms with 1-step power iteration and provides\n",
    "    a stable logK scalar ~ O(1) for CIFAR ResNets.\n",
    "\n",
    "    We keep u vectors in a separate dict (NOT optimizer.state) to avoid AdamW KeyError.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, eps: float = 1e-12, power_iter: int = 1):\n",
    "        self.eps = eps\n",
    "        self.power_iter = power_iter\n",
    "        self.params: List[torch.nn.Parameter] = []\n",
    "        self.shapes: List[Tuple[int, ...]] = []\n",
    "        self.fan_in: List[int] = []\n",
    "        self.u: Dict[int, torch.Tensor] = {}   # key: id(param)\n",
    "        self.v: Dict[int, torch.Tensor] = {}\n",
    "        self._collect(model)\n",
    "\n",
    "    def _collect(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if (p.ndim >= 2) and p.requires_grad:\n",
    "                # include conv + linear weights; skip biases and BN 1D params\n",
    "                self.params.append(p)\n",
    "                self.shapes.append(tuple(p.shape))\n",
    "                if p.ndim == 2:\n",
    "                    fin = p.shape[1]\n",
    "                else:\n",
    "                    # conv: (out, in, k, k)\n",
    "                    fin = p.shape[1] * int(np.prod(p.shape[2:]))\n",
    "                self.fan_in.append(int(fin))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reshape2d(self, p: torch.Tensor) -> torch.Tensor:\n",
    "        if p.ndim == 2:\n",
    "            return p\n",
    "        # conv weight -> (out, in*k*k)\n",
    "        return p.reshape(p.shape[0], -1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_uv_and_logK(self) -> float:\n",
    "        logs = []\n",
    "        for p, fin in zip(self.params, self.fan_in):\n",
    "            W = self._reshape2d(p).detach()\n",
    "            m, n = W.shape\n",
    "            key = id(p)\n",
    "\n",
    "            if key not in self.u:\n",
    "                # init u,v on same device/dtype as W\n",
    "                u = torch.randn(m, device=W.device, dtype=torch.float32)\n",
    "                u = u / (u.norm() + self.eps)\n",
    "                v = torch.randn(n, device=W.device, dtype=torch.float32)\n",
    "                v = v / (v.norm() + self.eps)\n",
    "                self.u[key], self.v[key] = u, v\n",
    "\n",
    "            u = self.u[key]\n",
    "            v = self.v[key]\n",
    "            # power iteration (float32 for stability)\n",
    "            W32 = W.float()\n",
    "            for _ in range(self.power_iter):\n",
    "                v = torch.mv(W32.t(), u)\n",
    "                v = v / (v.norm() + self.eps)\n",
    "                u = torch.mv(W32, v)\n",
    "                u = u / (u.norm() + self.eps)\n",
    "\n",
    "            sigma = torch.dot(u, torch.mv(W32, v)).abs().clamp_min(self.eps)\n",
    "            sigma_hat = sigma / math.sqrt(fin)  # normalized\n",
    "            logs.append(torch.log(sigma_hat + self.eps))\n",
    "\n",
    "            self.u[key], self.v[key] = u, v\n",
    "\n",
    "        logK = torch.stack(logs).mean().item()\n",
    "        return float(logK)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_penalty_grads(self, coeff: float):\n",
    "        \"\"\"\n",
    "        Adds coeff * d(logK)/dW to each weight gradient.\n",
    "        Using fixed (u,v) vectors (no backprop through power iteration).\n",
    "        \"\"\"\n",
    "        if coeff <= 0:\n",
    "            return\n",
    "\n",
    "        M = len(self.params)\n",
    "        for p, fin in zip(self.params, self.fan_in):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            W = self._reshape2d(p).detach().float()\n",
    "            key = id(p)\n",
    "            u = self.u[key]\n",
    "            v = self.v[key]\n",
    "\n",
    "            # sigma ~ u^T W v\n",
    "            sigma = torch.dot(u, torch.mv(W, v)).abs().clamp_min(self.eps)\n",
    "\n",
    "            # logK = mean_i log(sigma_i/sqrt(fin_i))\n",
    "            # d/dW log(sigma/sqrt(fin)) = (1/sigma) * u v^T\n",
    "            gW = (u[:, None] @ v[None, :]) / sigma\n",
    "            gW = gW / M  # because mean across layers\n",
    "\n",
    "            # map back to original shape\n",
    "            gW = gW.to(device=p.device, dtype=p.grad.dtype)\n",
    "            if p.ndim == 2:\n",
    "                p.grad.add_(coeff * gW)\n",
    "            else:\n",
    "                p.grad.add_(coeff * gW.reshape_as(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b41b122-b72b-4074-bdf7-d63724c65695",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ALMConfig:\n",
    "    warmup_epochs: int = 5           # let AdamW learn before constraints kick in\n",
    "    budget_delta: float = 0.50       # allowed slack above calibrated baseline\n",
    "    margin: float = 0.0              # can be negative to tighten, but start at 0\n",
    "\n",
    "    lam0: float = 0.0\n",
    "    rho0: float = 0.1\n",
    "\n",
    "    dual_lr: float = 0.02            # soft dual updates (prevents lam blowing up)\n",
    "    rho_growth: float = 1.3\n",
    "    rho_shrink: float = 0.9\n",
    "    rho_max: float = 2.0\n",
    "    lam_max: float = 3.0\n",
    "\n",
    "    ema_beta: float = 0.95\n",
    "    tol: float = 1e-3\n",
    "    patience: int = 2                # epochs of persistent violation before rho grows\n",
    "\n",
    "class SoftALMController:\n",
    "    \"\"\"\n",
    "    Controls a single scalar inequality g(W)=logK(W)-B <= 0\n",
    "    via projected dual ascent and safeguarded rho updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: ALMConfig):\n",
    "        self.cfg = cfg\n",
    "        self.lam = float(cfg.lam0)\n",
    "        self.rho = float(cfg.rho0)\n",
    "        self.g_ema = 0.0\n",
    "        self.B: Optional[float] = None\n",
    "        self._bad_epochs = 0\n",
    "        self._calib_logs: List[float] = []\n",
    "\n",
    "    def observe_logK(self, logK: float, epoch: int):\n",
    "        # warmup: collect stats for calibration\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            self._calib_logs.append(float(logK))\n",
    "            return\n",
    "\n",
    "        # on first post-warmup epoch, set budget B from warmup median\n",
    "        if self.B is None:\n",
    "            med = float(np.median(self._calib_logs)) if len(self._calib_logs) else float(logK)\n",
    "            self.B = med + self.cfg.budget_delta + self.cfg.margin\n",
    "\n",
    "        g = float(logK - self.B)\n",
    "        self.g_ema = self.cfg.ema_beta * self.g_ema + (1 - self.cfg.ema_beta) * g\n",
    "\n",
    "    def penalty_coeff(self, epoch: int) -> float:\n",
    "        if epoch < self.cfg.warmup_epochs or self.B is None:\n",
    "            return 0.0\n",
    "        # only penalize when violating (g>0)\n",
    "        gpos = max(0.0, self.g_ema)\n",
    "        return max(0.0, self.lam + self.rho * gpos)\n",
    "\n",
    "    def end_epoch_update(self, epoch: int):\n",
    "        if epoch < self.cfg.warmup_epochs or self.B is None:\n",
    "            return\n",
    "\n",
    "        g = self.g_ema\n",
    "\n",
    "        # projected dual ascent (soft step size)\n",
    "        self.lam = float(np.clip(self.lam + self.cfg.dual_lr * self.rho * g, 0.0, self.cfg.lam_max))\n",
    "\n",
    "        # rho adaptation (safeguarded)\n",
    "        if g > self.cfg.tol:\n",
    "            self._bad_epochs += 1\n",
    "            if self._bad_epochs >= self.cfg.patience:\n",
    "                self.rho = float(min(self.cfg.rho_max, self.rho * self.cfg.rho_growth))\n",
    "                self._bad_epochs = 0\n",
    "        else:\n",
    "            self._bad_epochs = 0\n",
    "            self.rho = float(max(self.cfg.rho0, self.rho * self.cfg.rho_shrink))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "237c0040-9d3d-4d86-a7a7-20435cd6561f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mutable default <class '__main__.ALMConfig'> for field alm is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTrainConfig\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1232\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1222\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:958\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    955\u001b[0m         kw_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;66;03m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m         cls_fields\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_get_field\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cls_fields:\n\u001b[1;32m    961\u001b[0m     fields[f\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m f\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:815\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# not the instance.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01mis\u001b[39;00m _FIELD \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdefault\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__hash__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmutable default \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f\u001b[38;5;241m.\u001b[39mdefault)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for field \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    816\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed: use default_factory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class '__main__.ALMConfig'> for field alm is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    depth: str = \"resnet18\"\n",
    "    epochs: int = 60\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.02\n",
    "    amp: bool = True\n",
    "\n",
    "    use_alm: bool = False\n",
    "    alm: ALMConfig = ALMConfig()\n",
    "    constraint_every: int = 20  # steps between logK measurements (cost control)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def train_one_run(seed: int, data_cfg: DataConfig, cfg: TrainConfig, device: str = \"cuda\", log_every: int = 10):\n",
    "    set_seed(seed)\n",
    "    train_loader, val_loader, test_loader, num_classes = make_loaders(data_cfg)\n",
    "\n",
    "    model = make_resnet_cifar(cfg.depth, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(cfg.amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    # Optimizer(s)\n",
    "    if not cfg.use_alm:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        tracker, ctrl = None, None\n",
    "        method = \"AdamW\"\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        tracker = SpectralTracker(model, power_iter=1)\n",
    "        ctrl = SoftALMController(cfg.alm)\n",
    "        method = \"ALM\"\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_test_at_best_val = -1.0\n",
    "\n",
    "    global_step = 0\n",
    "    hist = []\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        # epoch running logK updates (ALM only)\n",
    "        last_logK = float(\"nan\")\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=(cfg.amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # ALM: periodically measure constraint + add penalty grads\n",
    "            if cfg.use_alm and (global_step % cfg.constraint_every == 0):\n",
    "                last_logK = tracker.update_uv_and_logK()\n",
    "                ctrl.observe_logK(last_logK, epoch=ep-1)  # epoch index starting at 0\n",
    "\n",
    "            if cfg.use_alm:\n",
    "                coeff = ctrl.penalty_coeff(epoch=ep-1)\n",
    "                tracker.add_penalty_grads(coeff)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.numel()\n",
    "            global_step += 1\n",
    "\n",
    "        # end epoch eval\n",
    "        train_acc = correct / total\n",
    "        val_acc = eval_acc(model, val_loader, device)\n",
    "        test_acc = eval_acc(model, test_loader, device)\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_test_at_best_val = test_acc\n",
    "\n",
    "        # ALM dual update once per epoch\n",
    "        if cfg.use_alm:\n",
    "            ctrl.end_epoch_update(epoch=ep-1)\n",
    "\n",
    "        if (ep == 1) or (ep % log_every == 0) or (ep == cfg.epochs):\n",
    "            if not cfg.use_alm:\n",
    "                print(f\"[{method}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f}\")\n",
    "            else:\n",
    "                print(f\"[{method}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f} \"\n",
    "                      f\"logK={last_logK:.4f} g_ema={ctrl.g_ema:.6f} lam={ctrl.lam:.3f} rho={ctrl.rho:.3f} B={(ctrl.B if ctrl.B is not None else float('nan')):.4f}\")\n",
    "\n",
    "        hist.append({\n",
    "            \"seed\": seed, \"method\": method, \"epoch\": ep,\n",
    "            \"train_acc\": train_acc, \"val_acc\": val_acc, \"test_acc\": test_acc,\n",
    "            \"best_val_acc\": best_val, \"test_at_best_val\": best_test_at_best_val,\n",
    "            \"logK\": last_logK,\n",
    "            \"g_ema\": (ctrl.g_ema if cfg.use_alm else np.nan),\n",
    "            \"lam\": (ctrl.lam if cfg.use_alm else np.nan),\n",
    "            \"rho\": (ctrl.rho if cfg.use_alm else np.nan),\n",
    "            \"B\": (ctrl.B if (cfg.use_alm and ctrl.B is not None) else np.nan),\n",
    "        })\n",
    "\n",
    "    summ = {\n",
    "        \"seed\": seed, \"method\": method,\n",
    "        \"final_test_acc\": float(hist[-1][\"test_acc\"]),\n",
    "        \"best_val_acc\": float(best_val),\n",
    "        \"test_at_best_val\": float(best_test_at_best_val),\n",
    "    }\n",
    "    return pd.DataFrame(hist), summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16772a9-e558-4659-ae81-489b4eb97b3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainConfig.__init__() got an unexpected keyword argument 'alm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     27\u001b[0m data_cfg \u001b[38;5;241m=\u001b[39m DataConfig(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, val_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, split_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m     29\u001b[0m adam_cfg \u001b[38;5;241m=\u001b[39m TrainConfig(\n\u001b[1;32m     30\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     use_alm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m alm_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_alm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43malm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALMConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# start feasible\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlam0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdual_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrho_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_shrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrho_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m seeds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m     62\u001b[0m all_summ \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainConfig.__init__() got an unexpected keyword argument 'alm'"
     ]
    }
   ],
   "source": [
    "def bootstrap_ci(x, iters=5000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.array(x, dtype=float)\n",
    "    n = len(x)\n",
    "    means = []\n",
    "    for _ in range(iters):\n",
    "        samp = x[rng.randint(0, n, size=n)]\n",
    "        means.append(samp.mean())\n",
    "    means = np.sort(means)\n",
    "    return float(x.mean()), float(means[int(0.025*iters)]), float(means[int(0.975*iters)])\n",
    "\n",
    "def paired_permutation_pvalue(a, b, iters=5000, seed=0):\n",
    "    # H0: mean(a-b)=0\n",
    "    rng = np.random.RandomState(seed)\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    d = a - b\n",
    "    obs = abs(d.mean())\n",
    "    cnt = 0\n",
    "    for _ in range(iters):\n",
    "        signs = rng.choice([-1, 1], size=len(d))\n",
    "        stat = abs((d * signs).mean())\n",
    "        cnt += (stat >= obs)\n",
    "    return (cnt + 1) / (iters + 1)\n",
    "\n",
    "# === CONFIG YOU SHOULD USE (prevents saturation) ===\n",
    "data_cfg = DataConfig(dataset=\"cifar10\", batch_size=128, num_workers=4, val_frac=0.1, split_seed=123)\n",
    "\n",
    "adam_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "    use_alm=False\n",
    ")\n",
    "\n",
    "alm_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "    use_alm=True,\n",
    "    alm=ALMConfig(\n",
    "        warmup_epochs=5,\n",
    "        budget_delta=0.50,   # start feasible\n",
    "        margin=0.0,\n",
    "        lam0=0.0, rho0=0.1,\n",
    "        dual_lr=0.02,\n",
    "        rho_growth=1.3, rho_shrink=0.9,\n",
    "        rho_max=2.0, lam_max=3.0,\n",
    "        ema_beta=0.95,\n",
    "        tol=1e-3,\n",
    "        patience=2\n",
    "    ),\n",
    "    constraint_every=20\n",
    ")\n",
    "\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "all_summ = []\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"RUN seed=\", s, \"AdamW\")\n",
    "    _, summA = train_one_run(s, data_cfg, adam_cfg, device=device, log_every=10)\n",
    "    all_summ.append(summA)\n",
    "\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"RUN seed=\", s, \"ALM\")\n",
    "    _, summB = train_one_run(s, data_cfg, alm_cfg, device=device, log_every=10)\n",
    "    all_summ.append(summB)\n",
    "\n",
    "summ_df = pd.DataFrame(all_summ)\n",
    "display(summ_df)\n",
    "\n",
    "adam = [float(summ_df[(summ_df.seed==s) & (summ_df.method==\"AdamW\")][\"final_test_acc\"].iloc[0]) for s in seeds]\n",
    "alm  = [float(summ_df[(summ_df.seed==s) & (summ_df.method==\"ALM\")][\"final_test_acc\"].iloc[0]) for s in seeds]\n",
    "\n",
    "ma, lo_a, hi_a = bootstrap_ci(adam, seed=0)\n",
    "mb, lo_b, hi_b = bootstrap_ci(alm,  seed=1)\n",
    "pval = paired_permutation_pvalue(np.array(alm), np.array(adam), seed=2)\n",
    "\n",
    "print(\"\\nFINAL RESULTS (paired by seed)\")\n",
    "print(f\"AdamW test_acc: mean={ma:.4f}  95%CI=[{lo_a:.4f},{hi_a:.4f}]\")\n",
    "print(f\"ALM   test_acc: mean={mb:.4f}  95%CI=[{lo_b:.4f},{hi_b:.4f}]\")\n",
    "print(f\"Paired permutation p-value (ALM vs AdamW): {pval:.6f}\")\n",
    "print(f\"Mean diff (ALM-AdamW) = {(np.mean(alm)-np.mean(adam)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e39e6ff-b63c-4b42-b6d3-ec8064007454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    depth: str = \"resnet18\"\n",
    "    epochs: int = 60\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.02\n",
    "    amp: bool = True\n",
    "\n",
    "    use_alm: bool = False\n",
    "    alm: ALMConfig = field(default_factory=ALMConfig)   # <-- FIX: default_factory\n",
    "    constraint_every: int = 20  # steps between logK measurements\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def train_one_run(seed: int, data_cfg: DataConfig, cfg: TrainConfig, device: str = \"cuda\", log_every: int = 10):\n",
    "    set_seed(seed)\n",
    "    train_loader, val_loader, test_loader, num_classes = make_loaders(data_cfg)\n",
    "\n",
    "    model = make_resnet_cifar(cfg.depth, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(cfg.amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    if not cfg.use_alm:\n",
    "        tracker, ctrl = None, None\n",
    "        method = \"AdamW\"\n",
    "    else:\n",
    "        tracker = SpectralTracker(model, power_iter=1)\n",
    "        ctrl = SoftALMController(cfg.alm)\n",
    "        method = \"ALM\"\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_test_at_best_val = -1.0\n",
    "\n",
    "    global_step = 0\n",
    "    hist = []\n",
    "    last_logK = float(\"nan\")\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=(cfg.amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # ALM: periodically measure constraint + add penalty grads\n",
    "            if cfg.use_alm and (global_step % cfg.constraint_every == 0):\n",
    "                last_logK = tracker.update_uv_and_logK()\n",
    "                ctrl.observe_logK(last_logK, epoch=ep-1)\n",
    "\n",
    "            if cfg.use_alm:\n",
    "                coeff = ctrl.penalty_coeff(epoch=ep-1)\n",
    "                tracker.add_penalty_grads(coeff)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.numel()\n",
    "            global_step += 1\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = eval_acc(model, val_loader, device)\n",
    "        test_acc = eval_acc(model, test_loader, device)\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_test_at_best_val = test_acc\n",
    "\n",
    "        if cfg.use_alm:\n",
    "            ctrl.end_epoch_update(epoch=ep-1)\n",
    "\n",
    "        if (ep == 1) or (ep % log_every == 0) or (ep == cfg.epochs):\n",
    "            if not cfg.use_alm:\n",
    "                print(f\"[{method}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f}\")\n",
    "            else:\n",
    "                print(f\"[{method}|{data_cfg.dataset}|{cfg.depth}] ep={ep:03d} train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f} \"\n",
    "                      f\"logK={last_logK:.4f} g_ema={ctrl.g_ema:.6f} lam={ctrl.lam:.3f} rho={ctrl.rho:.3f} B={(ctrl.B if ctrl.B is not None else float('nan')):.4f}\")\n",
    "\n",
    "        hist.append({\n",
    "            \"seed\": seed, \"method\": method, \"epoch\": ep,\n",
    "            \"train_acc\": train_acc, \"val_acc\": val_acc, \"test_acc\": test_acc,\n",
    "            \"best_val_acc\": best_val, \"test_at_best_val\": best_test_at_best_val,\n",
    "            \"logK\": last_logK,\n",
    "            \"g_ema\": (ctrl.g_ema if cfg.use_alm else np.nan),\n",
    "            \"lam\": (ctrl.lam if cfg.use_alm else np.nan),\n",
    "            \"rho\": (ctrl.rho if cfg.use_alm else np.nan),\n",
    "            \"B\": (ctrl.B if (cfg.use_alm and ctrl.B is not None) else np.nan),\n",
    "        })\n",
    "\n",
    "    summ = {\n",
    "        \"seed\": seed, \"method\": method,\n",
    "        \"final_test_acc\": float(hist[-1][\"test_acc\"]),\n",
    "        \"best_val_acc\": float(best_val),\n",
    "        \"test_at_best_val\": float(best_test_at_best_val),\n",
    "    }\n",
    "    return pd.DataFrame(hist), summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69e6cc9c-205a-4a8d-a417-ed995fc971e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 1 AdamW\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[AdamW|cifar10|resnet18] ep=001 train_acc=0.4824 val_acc=0.5582 test_acc=0.5581\n",
      "[AdamW|cifar10|resnet18] ep=010 train_acc=0.8769 val_acc=0.8440 test_acc=0.8482\n",
      "[AdamW|cifar10|resnet18] ep=020 train_acc=0.9353 val_acc=0.8622 test_acc=0.8593\n",
      "[AdamW|cifar10|resnet18] ep=030 train_acc=0.9633 val_acc=0.8800 test_acc=0.8822\n",
      "[AdamW|cifar10|resnet18] ep=040 train_acc=0.9760 val_acc=0.8912 test_acc=0.8918\n",
      "[AdamW|cifar10|resnet18] ep=050 train_acc=0.9799 val_acc=0.9076 test_acc=0.9053\n",
      "[AdamW|cifar10|resnet18] ep=060 train_acc=0.9846 val_acc=0.9042 test_acc=0.9031\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 2 AdamW\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[AdamW|cifar10|resnet18] ep=001 train_acc=0.4871 val_acc=0.5540 test_acc=0.5604\n",
      "[AdamW|cifar10|resnet18] ep=010 train_acc=0.8769 val_acc=0.8214 test_acc=0.8225\n",
      "[AdamW|cifar10|resnet18] ep=020 train_acc=0.9364 val_acc=0.8730 test_acc=0.8800\n",
      "[AdamW|cifar10|resnet18] ep=030 train_acc=0.9622 val_acc=0.8916 test_acc=0.8894\n",
      "[AdamW|cifar10|resnet18] ep=040 train_acc=0.9759 val_acc=0.9056 test_acc=0.8964\n",
      "[AdamW|cifar10|resnet18] ep=050 train_acc=0.9817 val_acc=0.9042 test_acc=0.9028\n",
      "[AdamW|cifar10|resnet18] ep=060 train_acc=0.9854 val_acc=0.9004 test_acc=0.8963\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 3 AdamW\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[AdamW|cifar10|resnet18] ep=001 train_acc=0.4937 val_acc=0.6028 test_acc=0.6148\n",
      "[AdamW|cifar10|resnet18] ep=010 train_acc=0.8770 val_acc=0.8384 test_acc=0.8457\n",
      "[AdamW|cifar10|resnet18] ep=020 train_acc=0.9354 val_acc=0.8604 test_acc=0.8670\n",
      "[AdamW|cifar10|resnet18] ep=030 train_acc=0.9608 val_acc=0.8832 test_acc=0.8926\n",
      "[AdamW|cifar10|resnet18] ep=040 train_acc=0.9764 val_acc=0.9008 test_acc=0.9024\n",
      "[AdamW|cifar10|resnet18] ep=050 train_acc=0.9800 val_acc=0.9068 test_acc=0.8989\n",
      "[AdamW|cifar10|resnet18] ep=060 train_acc=0.9841 val_acc=0.9050 test_acc=0.9044\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 4 AdamW\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[AdamW|cifar10|resnet18] ep=001 train_acc=0.4854 val_acc=0.5404 test_acc=0.5454\n",
      "[AdamW|cifar10|resnet18] ep=010 train_acc=0.8733 val_acc=0.8378 test_acc=0.8429\n",
      "[AdamW|cifar10|resnet18] ep=020 train_acc=0.9352 val_acc=0.8764 test_acc=0.8743\n",
      "[AdamW|cifar10|resnet18] ep=030 train_acc=0.9635 val_acc=0.8822 test_acc=0.8791\n",
      "[AdamW|cifar10|resnet18] ep=040 train_acc=0.9744 val_acc=0.8974 test_acc=0.9004\n",
      "[AdamW|cifar10|resnet18] ep=050 train_acc=0.9829 val_acc=0.8974 test_acc=0.8993\n",
      "[AdamW|cifar10|resnet18] ep=060 train_acc=0.9865 val_acc=0.9116 test_acc=0.9081\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 5 AdamW\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[AdamW|cifar10|resnet18] ep=001 train_acc=0.4875 val_acc=0.5832 test_acc=0.5861\n",
      "[AdamW|cifar10|resnet18] ep=010 train_acc=0.8752 val_acc=0.8394 test_acc=0.8427\n",
      "[AdamW|cifar10|resnet18] ep=020 train_acc=0.9358 val_acc=0.8774 test_acc=0.8843\n",
      "[AdamW|cifar10|resnet18] ep=030 train_acc=0.9637 val_acc=0.8886 test_acc=0.8890\n",
      "[AdamW|cifar10|resnet18] ep=040 train_acc=0.9732 val_acc=0.9016 test_acc=0.8994\n",
      "[AdamW|cifar10|resnet18] ep=050 train_acc=0.9817 val_acc=0.9054 test_acc=0.9032\n",
      "[AdamW|cifar10|resnet18] ep=060 train_acc=0.9855 val_acc=0.9048 test_acc=0.9055\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 1 ALM\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4795 val_acc=0.5558 test_acc=0.5608 logK=-2.7410 g_ema=0.000000 lam=0.000 rho=0.100 B=nan\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8759 val_acc=0.8378 test_acc=0.8388 logK=-2.4448 g_ema=-0.302626 lam=0.000 rho=0.100 B=-2.1606\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9354 val_acc=0.8736 test_acc=0.8705 logK=-2.2866 g_ema=-0.140016 lam=0.000 rho=0.100 B=-2.1606\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9606 val_acc=0.8826 test_acc=0.8888 logK=-2.1792 g_ema=-0.029074 lam=0.000 rho=0.100 B=-2.1606\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9744 val_acc=0.8974 test_acc=0.8951 logK=-2.0982 g_ema=0.055190 lam=0.001 rho=0.220 B=-2.1606\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9799 val_acc=0.9062 test_acc=0.8989 logK=-2.0339 g_ema=0.119376 lam=0.010 rho=0.816 B=-2.1606\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9856 val_acc=0.9040 test_acc=0.9012 logK=-1.9851 g_ema=0.170984 lam=0.056 rho=2.000 B=-2.1606\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 2 ALM\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4862 val_acc=0.5642 test_acc=0.5637 logK=-2.7351 g_ema=0.000000 lam=0.000 rho=0.100 B=nan\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8775 val_acc=0.7952 test_acc=0.7983 logK=-2.4426 g_ema=-0.308803 lam=0.000 rho=0.100 B=-2.1503\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9356 val_acc=0.8740 test_acc=0.8842 logK=-2.2854 g_ema=-0.149984 lam=0.000 rho=0.100 B=-2.1503\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9613 val_acc=0.8900 test_acc=0.8955 logK=-2.1790 g_ema=-0.040160 lam=0.000 rho=0.100 B=-2.1503\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9750 val_acc=0.9078 test_acc=0.9000 logK=-2.1005 g_ema=0.043615 lam=0.000 rho=0.220 B=-2.1503\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9806 val_acc=0.9062 test_acc=0.8997 logK=-2.0364 g_ema=0.106723 lam=0.007 rho=0.816 B=-2.1503\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9854 val_acc=0.9084 test_acc=0.9055 logK=-1.9846 g_ema=0.160299 lam=0.048 rho=2.000 B=-2.1503\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 3 ALM\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4921 val_acc=0.6068 test_acc=0.6158 logK=-2.7434 g_ema=0.000000 lam=0.000 rho=0.100 B=nan\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8768 val_acc=0.8418 test_acc=0.8447 logK=-2.4407 g_ema=-0.305639 lam=0.000 rho=0.100 B=-2.1539\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9352 val_acc=0.8622 test_acc=0.8703 logK=-2.2802 g_ema=-0.141088 lam=0.000 rho=0.100 B=-2.1539\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9618 val_acc=0.8958 test_acc=0.8951 logK=-2.1735 g_ema=-0.028839 lam=0.000 rho=0.100 B=-2.1539\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9732 val_acc=0.8996 test_acc=0.8986 logK=-2.0928 g_ema=0.053504 lam=0.001 rho=0.286 B=-2.1539\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9782 val_acc=0.9060 test_acc=0.9037 logK=-2.0240 g_ema=0.122632 lam=0.011 rho=1.060 B=-2.1539\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9850 val_acc=0.8918 test_acc=0.8958 logK=-1.9724 g_ema=0.176440 lam=0.063 rho=2.000 B=-2.1539\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 4 ALM\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4845 val_acc=0.5302 test_acc=0.5375 logK=-2.7347 g_ema=0.000000 lam=0.000 rho=0.100 B=nan\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8740 val_acc=0.8448 test_acc=0.8480 logK=-2.4389 g_ema=-0.310434 lam=0.000 rho=0.100 B=-2.1467\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9327 val_acc=0.8844 test_acc=0.8811 logK=-2.2803 g_ema=-0.148943 lam=0.000 rho=0.100 B=-2.1467\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9635 val_acc=0.8818 test_acc=0.8844 logK=-2.1791 g_ema=-0.041985 lam=0.000 rho=0.100 B=-2.1467\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9745 val_acc=0.8956 test_acc=0.8996 logK=-2.1007 g_ema=0.038919 lam=0.000 rho=0.220 B=-2.1467\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9829 val_acc=0.8978 test_acc=0.9020 logK=-2.0364 g_ema=0.104476 lam=0.007 rho=0.816 B=-2.1467\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9857 val_acc=0.9056 test_acc=0.9043 logK=-1.9838 g_ema=0.156682 lam=0.046 rho=2.000 B=-2.1467\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RUN seed= 5 ALM\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.4886 val_acc=0.5922 test_acc=0.5954 logK=-2.7395 g_ema=0.000000 lam=0.000 rho=0.100 B=nan\n",
      "[ALM|cifar10|resnet18] ep=010 train_acc=0.8775 val_acc=0.8386 test_acc=0.8380 logK=-2.4347 g_ema=-0.300865 lam=0.000 rho=0.100 B=-2.1536\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9352 val_acc=0.8798 test_acc=0.8862 logK=-2.2760 g_ema=-0.138046 lam=0.000 rho=0.100 B=-2.1536\n",
      "[ALM|cifar10|resnet18] ep=030 train_acc=0.9610 val_acc=0.8874 test_acc=0.8891 logK=-2.1694 g_ema=-0.026159 lam=0.000 rho=0.100 B=-2.1536\n",
      "[ALM|cifar10|resnet18] ep=040 train_acc=0.9760 val_acc=0.8834 test_acc=0.8885 logK=-2.0913 g_ema=0.053896 lam=0.001 rho=0.220 B=-2.1536\n",
      "[ALM|cifar10|resnet18] ep=050 train_acc=0.9811 val_acc=0.9008 test_acc=0.8983 logK=-2.0284 g_ema=0.118850 lam=0.010 rho=0.816 B=-2.1536\n",
      "[ALM|cifar10|resnet18] ep=060 train_acc=0.9852 val_acc=0.9018 test_acc=0.8994 logK=-1.9739 g_ema=0.173044 lam=0.057 rho=2.000 B=-2.1536\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>method</th>\n",
       "      <th>final_test_acc</th>\n",
       "      <th>best_val_acc</th>\n",
       "      <th>test_at_best_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.9013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.9044</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.9043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>0.9081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.9106</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>ALM</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>ALM</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.9052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>ALM</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>0.9102</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>ALM</td>\n",
       "      <td>0.9043</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.9043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>ALM</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>0.9077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed method  final_test_acc  best_val_acc  test_at_best_val\n",
       "0     1  AdamW          0.9031        0.9096            0.9013\n",
       "1     2  AdamW          0.8963        0.9082            0.9042\n",
       "2     3  AdamW          0.9044        0.9096            0.9043\n",
       "3     4  AdamW          0.9081        0.9116            0.9081\n",
       "4     5  AdamW          0.9055        0.9106            0.9024\n",
       "5     1    ALM          0.9012        0.9130            0.9141\n",
       "6     2    ALM          0.9055        0.9162            0.9052\n",
       "7     3    ALM          0.8958        0.9102            0.9034\n",
       "8     4    ALM          0.9043        0.9092            0.9043\n",
       "9     5    ALM          0.8994        0.9104            0.9077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL RESULTS (paired by seed)\n",
      "AdamW test_acc: mean=0.9035  95%CI=[0.8995,0.9065]\n",
      "ALM   test_acc: mean=0.9012  95%CI=[0.8980,0.9042]\n",
      "Paired permutation p-value (ALM vs AdamW): 0.507898\n",
      "Mean diff (ALM-AdamW) = -0.0022\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_ci(x, iters=5000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.array(x, dtype=float)\n",
    "    n = len(x)\n",
    "    means = []\n",
    "    for _ in range(iters):\n",
    "        samp = x[rng.randint(0, n, size=n)]\n",
    "        means.append(samp.mean())\n",
    "    means = np.sort(means)\n",
    "    return float(x.mean()), float(means[int(0.025*iters)]), float(means[int(0.975*iters)])\n",
    "\n",
    "def paired_permutation_pvalue(a, b, iters=5000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    d = a - b\n",
    "    obs = abs(d.mean())\n",
    "    cnt = 0\n",
    "    for _ in range(iters):\n",
    "        signs = rng.choice([-1, 1], size=len(d))\n",
    "        stat = abs((d * signs).mean())\n",
    "        cnt += (stat >= obs)\n",
    "    return (cnt + 1) / (iters + 1)\n",
    "\n",
    "data_cfg = DataConfig(dataset=\"cifar10\", batch_size=128, num_workers=4, val_frac=0.1, split_seed=123)\n",
    "\n",
    "adam_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "    use_alm=False\n",
    ")\n",
    "\n",
    "alm_cfg = TrainConfig(\n",
    "    depth=\"resnet18\",\n",
    "    epochs=60,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.02,\n",
    "    amp=True,\n",
    "    use_alm=True,\n",
    "    alm=ALMConfig(\n",
    "        warmup_epochs=5,\n",
    "        budget_delta=0.50,\n",
    "        margin=0.0,\n",
    "        lam0=0.0, rho0=0.1,\n",
    "        dual_lr=0.02,\n",
    "        rho_growth=1.3, rho_shrink=0.9,\n",
    "        rho_max=2.0, lam_max=3.0,\n",
    "        ema_beta=0.95,\n",
    "        tol=1e-3,\n",
    "        patience=2\n",
    "    ),\n",
    "    constraint_every=20\n",
    ")\n",
    "\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "all_summ = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"RUN seed=\", s, \"AdamW\")\n",
    "    _, summA = train_one_run(s, data_cfg, adam_cfg, device=device, log_every=10)\n",
    "    all_summ.append(summA)\n",
    "\n",
    "for s in seeds:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"RUN seed=\", s, \"ALM\")\n",
    "    _, summB = train_one_run(s, data_cfg, alm_cfg, device=device, log_every=10)\n",
    "    all_summ.append(summB)\n",
    "\n",
    "summ_df = pd.DataFrame(all_summ)\n",
    "display(summ_df)\n",
    "\n",
    "adam = [float(summ_df[(summ_df.seed==s) & (summ_df.method==\"AdamW\")][\"final_test_acc\"].iloc[0]) for s in seeds]\n",
    "alm  = [float(summ_df[(summ_df.seed==s) & (summ_df.method==\"ALM\")][\"final_test_acc\"].iloc[0]) for s in seeds]\n",
    "\n",
    "ma, lo_a, hi_a = bootstrap_ci(adam, seed=0)\n",
    "mb, lo_b, hi_b = bootstrap_ci(alm,  seed=1)\n",
    "pval = paired_permutation_pvalue(np.array(alm), np.array(adam), seed=2)\n",
    "\n",
    "print(\"\\nFINAL RESULTS (paired by seed)\")\n",
    "print(f\"AdamW test_acc: mean={ma:.4f}  95%CI=[{lo_a:.4f},{hi_a:.4f}]\")\n",
    "print(f\"ALM   test_acc: mean={mb:.4f}  95%CI=[{lo_b:.4f},{hi_b:.4f}]\")\n",
    "print(f\"Paired permutation p-value (ALM vs AdamW): {pval:.6f}\")\n",
    "print(f\"Mean diff (ALM-AdamW) = {(np.mean(alm)-np.mean(adam)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5284d1d6-d752-4392-a00b-d2fbd5f0f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U timm scipy pandas matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d87937-e318-46a3-bc16-a903f4f5461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: tighter reproducibility (may reduce throughput)\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def bootstrap_ci(x, iters=5000, alpha=0.05, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = len(x)\n",
    "    means = []\n",
    "    for _ in range(iters):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        means.append(x[idx].mean())\n",
    "    means = np.sort(means)\n",
    "    lo = means[int((alpha/2)*iters)]\n",
    "    hi = means[int((1-alpha/2)*iters)]\n",
    "    return float(x.mean()), float(lo), float(hi)\n",
    "\n",
    "def paired_permutation_test(a, b, iters=10000, seed=0):\n",
    "    # H0: E[a-b]=0\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    d = a - b\n",
    "    obs = abs(d.mean())\n",
    "    cnt = 0\n",
    "    for _ in range(iters):\n",
    "        signs = rng.choice([-1, 1], size=len(d))\n",
    "        stat = abs((d * signs).mean())\n",
    "        cnt += (stat >= obs)\n",
    "    return (cnt + 1) / (iters + 1)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df523e5-89f1-4590-9f0f-6f9aca7f5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = \"cifar10\"        # \"cifar10\" or \"cifar100\"\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    val_frac: float = 0.1\n",
    "    split_seed: int = 123\n",
    "\n",
    "    # robustness knobs\n",
    "    label_noise: float = 0.0        # fraction in [0,1]\n",
    "    randaugment_N: int = 0          # 0 disables\n",
    "    randaugment_M: int = 0          # 0 disables\n",
    "\n",
    "def make_transforms(randaugment_N=0, randaugment_M=0):\n",
    "    # Standard CIFAR augmentation\n",
    "    train_tf = [\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    if randaugment_N > 0:\n",
    "        # Torchvision RandAugment expects PIL images; CIFAR datasets return PIL\n",
    "        train_tf.append(T.RandAugment(num_ops=randaugment_N, magnitude=randaugment_M))\n",
    "    train_tf += [T.ToTensor(),\n",
    "                 T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))]\n",
    "    test_tf = [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
    "    ]\n",
    "    return T.Compose(train_tf), T.Compose(test_tf)\n",
    "\n",
    "class LabelNoised(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds, num_classes, noise=0.0, seed=0):\n",
    "        self.base = base_ds\n",
    "        self.num_classes = num_classes\n",
    "        self.noise = float(noise)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        # build noisy labels once\n",
    "        self.labels = []\n",
    "        for i in range(len(base_ds)):\n",
    "            _, y = base_ds[i]\n",
    "            if self.noise > 0 and self.rng.random() < self.noise:\n",
    "                y = int(self.rng.integers(0, num_classes))\n",
    "            self.labels.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.base[idx]\n",
    "        return x, self.labels[idx]\n",
    "\n",
    "def get_dataloaders(cfg: DataConfig):\n",
    "    train_tf, test_tf = make_transforms(cfg.randaugment_N, cfg.randaugment_M)\n",
    "\n",
    "    if cfg.dataset.lower() == \"cifar10\":\n",
    "        train_ds = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tf)\n",
    "        test_ds  = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tf)\n",
    "        num_classes = 10\n",
    "    elif cfg.dataset.lower() == \"cifar100\":\n",
    "        train_ds = torchvision.datasets.CIFAR100(\"./data\", train=True, download=True, transform=train_tf)\n",
    "        test_ds  = torchvision.datasets.CIFAR100(\"./data\", train=False, download=True, transform=test_tf)\n",
    "        num_classes = 100\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be cifar10 or cifar100\")\n",
    "\n",
    "    # label noise applies only to training set (incl. validation split)\n",
    "    if cfg.label_noise > 0:\n",
    "        train_ds = LabelNoised(train_ds, num_classes=num_classes, noise=cfg.label_noise, seed=cfg.split_seed)\n",
    "\n",
    "    n = len(train_ds)\n",
    "    n_val = int(cfg.val_frac * n)\n",
    "    n_tr  = n - n_val\n",
    "    g = torch.Generator().manual_seed(cfg.split_seed)\n",
    "    tr_ds, va_ds = random_split(train_ds, [n_tr, n_val], generator=g)\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    va_loader = DataLoader(va_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    te_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return tr_loader, va_loader, te_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40d718f-1729-439b-8f51-9cb1ac21d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CIFAR ResNet (He et al. style: 3x3 stem, no maxpool) ----\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        return out\n",
    "\n",
    "class CIFARResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = conv3x3(3, 64, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes*block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.pool(out).flatten(1)\n",
    "        return self.fc(out)\n",
    "\n",
    "def resnet18_cifar(num_classes): return CIFARResNet(BasicBlock, [2,2,2,2], num_classes)\n",
    "def resnet34_cifar(num_classes): return CIFARResNet(BasicBlock, [3,4,6,3], num_classes)\n",
    "\n",
    "# ---- WideResNet-28-10 (standard CIFAR family) ----\n",
    "class WRNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x), inplace=True)\n",
    "        shortcut = x if self.shortcut is None else self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = F.relu(self.bn2(out), inplace=True)\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return out + shortcut\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, num_classes=10, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        k = widen_factor\n",
    "        stages = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, stages[0], 3, 1, 1, bias=False)\n",
    "        self.block1 = self._make_group(stages[0], stages[1], n, stride=1, drop_rate=drop_rate)\n",
    "        self.block2 = self._make_group(stages[1], stages[2], n, stride=2, drop_rate=drop_rate)\n",
    "        self.block3 = self._make_group(stages[2], stages[3], n, stride=2, drop_rate=drop_rate)\n",
    "        self.bn = nn.BatchNorm2d(stages[3])\n",
    "        self.fc = nn.Linear(stages[3], num_classes)\n",
    "\n",
    "    def _make_group(self, in_ch, out_ch, n, stride, drop_rate):\n",
    "        layers = [WRNBlock(in_ch, out_ch, stride, drop_rate)]\n",
    "        for _ in range(n-1):\n",
    "            layers.append(WRNBlock(out_ch, out_ch, 1, drop_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = F.relu(self.bn(out), inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, 1).flatten(1)\n",
    "        return self.fc(out)\n",
    "\n",
    "def build_model(name: str, num_classes: int):\n",
    "    name = name.lower()\n",
    "    if name == \"resnet18\": return resnet18_cifar(num_classes)\n",
    "    if name == \"resnet34\": return resnet34_cifar(num_classes)\n",
    "    if name == \"wrn28_10\":  return WideResNet(depth=28, widen_factor=10, num_classes=num_classes, drop_rate=0.0)\n",
    "    raise ValueError(\"model must be resnet18, resnet34, wrn28_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6d3277-161a-4074-b981-1a6ee7034f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralTracker:\n",
    "    def __init__(self, model: nn.Module, power_iter: int = 1, eps: float = 1e-12):\n",
    "        self.power_iter = int(power_iter)\n",
    "        self.eps = eps\n",
    "        self.buffers: Dict[int, Dict[str, torch.Tensor]] = {}\n",
    "        self.params = []\n",
    "        for p in model.parameters():\n",
    "            if p.ndim in (2, 4) and p.requires_grad:\n",
    "                self.params.append(p)\n",
    "\n",
    "    def _mat(self, W: torch.Tensor) -> torch.Tensor:\n",
    "        # conv: (out,in,kh,kw) -> (out, in*kh*kw)\n",
    "        if W.ndim == 4:\n",
    "            return W.reshape(W.shape[0], -1)\n",
    "        return W\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_uv(self, Wm: torch.Tensor, key: int, device: str):\n",
    "        out_dim = Wm.shape[0]\n",
    "        in_dim  = Wm.shape[1]\n",
    "        u = torch.randn(out_dim, device=device)\n",
    "        v = torch.randn(in_dim,  device=device)\n",
    "        u = u / (u.norm() + self.eps)\n",
    "        v = v / (v.norm() + self.eps)\n",
    "        self.buffers[key] = {\"u\": u, \"v\": v}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _power_iter(self, Wm: torch.Tensor, key: int):\n",
    "        u = self.buffers[key][\"u\"]\n",
    "        v = self.buffers[key][\"v\"]\n",
    "        for _ in range(self.power_iter):\n",
    "            v = torch.mv(Wm.t(), u)\n",
    "            v = v / (v.norm() + self.eps)\n",
    "            u = torch.mv(Wm, v)\n",
    "            u = u / (u.norm() + self.eps)\n",
    "        self.buffers[key][\"u\"] = u\n",
    "        self.buffers[key][\"v\"] = v\n",
    "\n",
    "    def logK(self, device: str) -> torch.Tensor:\n",
    "        # returns scalar tensor with grad\n",
    "        logsigmas = []\n",
    "        for p in self.params:\n",
    "            Wm = self._mat(p)\n",
    "            key = id(p)\n",
    "            if key not in self.buffers:\n",
    "                self._init_uv(Wm, key, device=device)\n",
    "            # update u,v without grad\n",
    "            self._power_iter(Wm.detach(), key)\n",
    "            u = self.buffers[key][\"u\"].detach()\n",
    "            v = self.buffers[key][\"v\"].detach()\n",
    "            sigma = torch.dot(u, torch.mv(Wm, v)).abs() + 1e-12\n",
    "            logsigmas.append(torch.log(sigma))\n",
    "        return torch.stack(logsigmas).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abdd4e10-267f-4f15-a8d8-417c1d44c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ALMConfig:\n",
    "    warmup_epochs: int = 5\n",
    "    budget_delta: float = 0.50      # slack: B = median(logK_warmup) + log(1+delta)\n",
    "    margin: float = 0.0             # optional additive slack in log space\n",
    "    lam0: float = 0.0\n",
    "    rho0: float = 0.1\n",
    "    dual_lr: float = 1.0            # multiplier on lambda update\n",
    "    rho_growth: float = 1.4\n",
    "    rho_shrink: float = 0.9\n",
    "    rho_max: float = 2.0\n",
    "    lam_max: float = 5.0\n",
    "    ema_beta: float = 0.95\n",
    "    tol: float = 1e-3\n",
    "    patience: int = 2\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model: str = \"resnet18\"\n",
    "    epochs: int = 200\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.02\n",
    "    amp: bool = True\n",
    "\n",
    "    method: str = \"adamw\"           # \"adamw\", \"sgd\", \"alm_adamw\", \"alm_sgd\"\n",
    "    momentum: float = 0.9           # for SGD\n",
    "    nesterov: bool = True\n",
    "    warmup_epochs: int = 5\n",
    "    label_smoothing: float = 0.0\n",
    "\n",
    "    # ALM\n",
    "    alm: ALMConfig = field(default_factory=ALMConfig)\n",
    "    constraint_every: int = 20      # steps frequency for applying ALM penalty\n",
    "    power_iter: int = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7843f467-592b-46f9-bd75-ce138f7798a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model, cfg: TrainConfig):\n",
    "    if cfg.method in (\"adamw\", \"alm_adamw\"):\n",
    "        return torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9, 0.999))\n",
    "    if cfg.method in (\"sgd\", \"alm_sgd\"):\n",
    "        return torch.optim.SGD(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "                               momentum=cfg.momentum, nesterov=cfg.nesterov)\n",
    "    raise ValueError(\"unknown method\")\n",
    "\n",
    "def make_scheduler(optimizer, total_epochs, warmup_epochs):\n",
    "    # Cosine decay to ~0, with linear warmup\n",
    "    def lr_lambda(ep):\n",
    "        if ep < warmup_epochs:\n",
    "            return (ep + 1) / max(1, warmup_epochs)\n",
    "        t = (ep - warmup_epochs) / max(1, total_epochs - warmup_epochs)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91325bbd-cd70-427d-8b51-28aa2817f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loop(model, loader, device):\n",
    "    model.eval()\n",
    "    tot, corr, loss_sum = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y, reduction=\"sum\")\n",
    "        loss_sum += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        corr += (pred == y).sum().item()\n",
    "        tot += y.numel()\n",
    "    return corr / tot, loss_sum / tot\n",
    "\n",
    "def train_one_run(seed: int, data_cfg: DataConfig, cfg: TrainConfig, device=\"cuda\", log_every=10):\n",
    "    seed_all(seed)\n",
    "\n",
    "    tr_loader, va_loader, te_loader, num_classes = get_dataloaders(data_cfg)\n",
    "    model = build_model(cfg.model, num_classes).to(device)\n",
    "\n",
    "    opt = make_optimizer(model, cfg)\n",
    "    sch = make_scheduler(opt, total_epochs=cfg.epochs, warmup_epochs=cfg.warmup_epochs)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(cfg.amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    # loss fn (optional label smoothing)\n",
    "    def ce_loss(logits, y):\n",
    "        if cfg.label_smoothing > 0:\n",
    "            return F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
    "        return F.cross_entropy(logits, y)\n",
    "\n",
    "    # ALM state\n",
    "    use_alm = cfg.method.startswith(\"alm_\")\n",
    "    tracker = SpectralTracker(model, power_iter=cfg.power_iter) if use_alm else None\n",
    "\n",
    "    lam = float(cfg.alm.lam0)\n",
    "    rho = float(cfg.alm.rho0)\n",
    "    g_ema = 0.0\n",
    "    B = None\n",
    "    warmup_logKs = []\n",
    "    bad_epochs = 0\n",
    "\n",
    "    step = 0\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    best_test_at_val = None\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        tot, corr = 0, 0\n",
    "        loss_sum = 0.0\n",
    "\n",
    "        for x, y in tr_loader:\n",
    "            step += 1\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(cfg.amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                base_loss = ce_loss(logits, y)\n",
    "\n",
    "                # default: no ALM penalty this step\n",
    "                aug_loss = base_loss\n",
    "                logK_val = None\n",
    "                g_pos = None\n",
    "\n",
    "                if use_alm and (step % cfg.constraint_every == 0):\n",
    "                    logK_val = tracker.logK(device=device)\n",
    "\n",
    "                    if ep <= cfg.alm.warmup_epochs:\n",
    "                        warmup_logKs.append(float(logK_val.detach().cpu()))\n",
    "                    else:\n",
    "                        if B is None:\n",
    "                            # Calibrate B from warmup distribution\n",
    "                            med = float(np.median(warmup_logKs)) if len(warmup_logKs) else float(logK_val.detach().cpu())\n",
    "                            # slack in log-space: log(1+delta) + margin\n",
    "                            B = med + math.log(1.0 + float(cfg.alm.budget_delta)) + float(cfg.alm.margin)\n",
    "\n",
    "                        g = logK_val - float(B)\n",
    "                        g_pos = torch.clamp(g, min=0.0)   # [g]_+\n",
    "                        # AL objective\n",
    "                        aug_loss = base_loss + (lam * g_pos) + 0.5 * rho * (g_pos ** 2)\n",
    "\n",
    "            scaler.scale(aug_loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            pred = logits.argmax(dim=1)\n",
    "            corr += (pred == y).sum().item()\n",
    "            tot += y.numel()\n",
    "            loss_sum += float(base_loss.detach().cpu()) * y.numel()\n",
    "\n",
    "            # Track constraint EMA and update dual on-the-fly (stable, bounded)\n",
    "            if use_alm and (step % cfg.constraint_every == 0) and (ep > cfg.alm.warmup_epochs) and (B is not None):\n",
    "                gp = float(g_pos.detach().cpu())\n",
    "                g_ema = cfg.alm.ema_beta * g_ema + (1.0 - cfg.alm.ema_beta) * gp\n",
    "\n",
    "        # Epoch end eval\n",
    "        train_acc = corr / tot\n",
    "        val_acc, val_loss = eval_loop(model, va_loader, device)\n",
    "        test_acc, test_loss = eval_loop(model, te_loader, device)\n",
    "\n",
    "        # Dual / penalty update once per epoch (KKT-ish outer loop)\n",
    "        if use_alm and (ep > cfg.alm.warmup_epochs) and (B is not None):\n",
    "            # Lambda update (project to >=0)\n",
    "            lam = max(0.0, min(cfg.alm.lam_max, lam + cfg.alm.dual_lr * rho * g_ema))\n",
    "\n",
    "            # Adaptive rho: if violation not decreasing, grow; else shrink mildly\n",
    "            if g_ema > cfg.alm.tol:\n",
    "                bad_epochs += 1\n",
    "            else:\n",
    "                bad_epochs = 0\n",
    "\n",
    "            if bad_epochs >= cfg.alm.patience:\n",
    "                rho = min(cfg.alm.rho_max, rho * cfg.alm.rho_growth)\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                rho = max(1e-8, rho * cfg.alm.rho_shrink)\n",
    "\n",
    "        sch.step()\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        # checkpoint by val\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_test_at_val = test_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        # Logging\n",
    "        row = dict(\n",
    "            seed=seed, method=cfg.method, dataset=data_cfg.dataset, model=cfg.model,\n",
    "            epoch=ep, train_acc=train_acc, val_acc=val_acc, test_acc=test_acc,\n",
    "            lr=float(opt.param_groups[0][\"lr\"]),\n",
    "            logK=float(logK_val.detach().cpu()) if logK_val is not None else np.nan,\n",
    "            g_ema=float(g_ema) if use_alm else np.nan,\n",
    "            lam=float(lam) if use_alm else np.nan,\n",
    "            rho=float(rho) if use_alm else np.nan,\n",
    "            B=float(B) if (B is not None) else np.nan,\n",
    "            seconds=dt\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "        if ep == 1 or ep % log_every == 0 or ep == cfg.epochs:\n",
    "            if use_alm:\n",
    "                print(f\"[{cfg.method.upper()}|{data_cfg.dataset}|{cfg.model}] ep={ep:03d} \"\n",
    "                      f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f} \"\n",
    "                      f\"logK={row['logK']:.4f} g_ema={row['g_ema']:.6f} lam={lam:.3f} rho={rho:.3f}\")\n",
    "            else:\n",
    "                print(f\"[{cfg.method.upper()}|{data_cfg.dataset}|{cfg.model}] ep={ep:03d} \"\n",
    "                      f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f}\")\n",
    "\n",
    "    # restore best-by-val for reporting\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    final_test_acc, _ = eval_loop(model, te_loader, device)\n",
    "\n",
    "    hist = pd.DataFrame(rows)\n",
    "    summary = dict(\n",
    "        seed=seed, method=cfg.method, dataset=data_cfg.dataset, model=cfg.model,\n",
    "        final_test_acc=float(final_test_acc),\n",
    "        best_val_acc=float(best_val),\n",
    "        test_at_best_val=float(best_test_at_val if best_test_at_val is not None else final_test_acc)\n",
    "    )\n",
    "    return hist, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af225821-74eb-40a1-8aa6-af7a6ced4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sweep:\n",
    "    seeds: List[int] = field(default_factory=lambda: [1,2,3,4,5])\n",
    "    epochs: int = 200\n",
    "\n",
    "    # Baseline grids (small but meaningful)\n",
    "    adamw_lr: Tuple[float,...] = (3e-4, 5e-4, 8e-4)\n",
    "    adamw_wd: Tuple[float,...] = (0.02, 0.05, 0.1)\n",
    "\n",
    "    sgd_lr: Tuple[float,...] = (0.05, 0.1, 0.2)\n",
    "    sgd_wd: Tuple[float,...] = (5e-4, 1e-3)\n",
    "\n",
    "    # ALM knobs (keep tight; ablate later)\n",
    "    budget_delta: Tuple[float,...] = (0.2, 0.5)\n",
    "    rho0: Tuple[float,...] = (0.05, 0.1)\n",
    "    dual_lr: Tuple[float,...] = (0.5, 1.0)\n",
    "\n",
    "def tune_one_seed(data_cfg, model_name, base_method, sweep: Sweep, device):\n",
    "    # quick tuning: fewer epochs, single seed for each candidate\n",
    "    tune_epochs = 60\n",
    "    seed = 0\n",
    "\n",
    "    candidates = []\n",
    "    if base_method == \"adamw\":\n",
    "        for lr in sweep.adamw_lr:\n",
    "            for wd in sweep.adamw_wd:\n",
    "                cfg = TrainConfig(model=model_name, epochs=tune_epochs, lr=lr, weight_decay=wd, method=\"adamw\")\n",
    "                _, summ = train_one_run(seed, data_cfg, cfg, device=device, log_every=20)\n",
    "                candidates.append((summ[\"best_val_acc\"], dict(lr=lr, wd=wd)))\n",
    "    elif base_method == \"sgd\":\n",
    "        for lr in sweep.sgd_lr:\n",
    "            for wd in sweep.sgd_wd:\n",
    "                cfg = TrainConfig(model=model_name, epochs=tune_epochs, lr=lr, weight_decay=wd, method=\"sgd\")\n",
    "                _, summ = train_one_run(seed, data_cfg, cfg, device=device, log_every=20)\n",
    "                candidates.append((summ[\"best_val_acc\"], dict(lr=lr, wd=wd)))\n",
    "    else:\n",
    "        raise ValueError(\"base_method must be adamw or sgd\")\n",
    "\n",
    "    best = sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
    "    print(\"TUNE BEST\", base_method, \":\", best)\n",
    "    return best\n",
    "\n",
    "def tune_alm_one_seed(data_cfg, model_name, base_method, sweep: Sweep, device):\n",
    "    tune_epochs = 60\n",
    "    seed = 0\n",
    "\n",
    "    base_lr_wd = tune_one_seed(data_cfg, model_name, base_method, sweep, device)\n",
    "\n",
    "    candidates = []\n",
    "    for bd in sweep.budget_delta:\n",
    "        for rho0 in sweep.rho0:\n",
    "            for dlr in sweep.dual_lr:\n",
    "                alm = ALMConfig(\n",
    "                    warmup_epochs=5,\n",
    "                    budget_delta=bd,\n",
    "                    margin=0.0,\n",
    "                    lam0=0.0, rho0=rho0,\n",
    "                    dual_lr=dlr,\n",
    "                    rho_growth=1.4, rho_shrink=0.9,\n",
    "                    rho_max=2.0, lam_max=5.0,\n",
    "                    ema_beta=0.95, tol=1e-3, patience=2\n",
    "                )\n",
    "                method = \"alm_adamw\" if base_method==\"adamw\" else \"alm_sgd\"\n",
    "                cfg = TrainConfig(\n",
    "                    model=model_name, epochs=tune_epochs,\n",
    "                    lr=base_lr_wd[\"lr\"], weight_decay=base_lr_wd[\"wd\"],\n",
    "                    method=method, alm=alm,\n",
    "                    constraint_every=20, power_iter=1\n",
    "                )\n",
    "                _, summ = train_one_run(seed, data_cfg, cfg, device=device, log_every=20)\n",
    "                candidates.append((summ[\"best_val_acc\"], dict(budget_delta=bd, rho0=rho0, dual_lr=dlr)))\n",
    "\n",
    "    best_alm = sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
    "    print(\"TUNE BEST ALM\", base_method, \":\", best_alm)\n",
    "    return base_lr_wd, best_alm\n",
    "\n",
    "def final_multi_seed(data_cfg, model_name, method_cfgs: Dict[str, TrainConfig], sweep: Sweep, device):\n",
    "    all_hist = []\n",
    "    all_summ = []\n",
    "    for seed in sweep.seeds:\n",
    "        for tag, cfg in method_cfgs.items():\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(\"RUN seed=\", seed, tag)\n",
    "            cfg2 = cfg\n",
    "            cfg2.epochs = sweep.epochs\n",
    "            hist, summ = train_one_run(seed, data_cfg, cfg2, device=device, log_every=10)\n",
    "            summ[\"tag\"] = tag\n",
    "            all_hist.append(hist)\n",
    "            all_summ.append(summ)\n",
    "    return pd.concat(all_hist, ignore_index=True), pd.DataFrame(all_summ)\n",
    "\n",
    "def paired_stats(summ_df: pd.DataFrame, a_tag: str, b_tag: str):\n",
    "    # pair by seed\n",
    "    a = []\n",
    "    b = []\n",
    "    for s in sorted(summ_df[\"seed\"].unique()):\n",
    "        a.append(float(summ_df[(summ_df.seed==s) & (summ_df.tag==a_tag)][\"final_test_acc\"].iloc[0]))\n",
    "        b.append(float(summ_df[(summ_df.seed==s) & (summ_df.tag==b_tag)][\"final_test_acc\"].iloc[0]))\n",
    "    mean_a, lo_a, hi_a = bootstrap_ci(a, seed=0)\n",
    "    mean_b, lo_b, hi_b = bootstrap_ci(b, seed=1)\n",
    "    pval = paired_permutation_test(b, a, seed=2)  # H1: b != a (two-sided)\n",
    "    diff = float(np.mean(np.array(b) - np.array(a)))\n",
    "    return dict(\n",
    "        a_tag=a_tag, b_tag=b_tag,\n",
    "        mean_a=mean_a, ci_a=(lo_a,hi_a),\n",
    "        mean_b=mean_b, ci_b=(lo_b,hi_b),\n",
    "        mean_diff=diff, pval=pval,\n",
    "        a_list=a, b_list=b\n",
    "    )\n",
    "\n",
    "def plot_final_bars(summ_df, title, outpath=None):\n",
    "    # mean test acc by tag\n",
    "    tags = sorted(summ_df[\"tag\"].unique())\n",
    "    means = []\n",
    "    cis = []\n",
    "    for t in tags:\n",
    "        vals = summ_df[summ_df.tag==t][\"final_test_acc\"].astype(float).values\n",
    "        m, lo, hi = bootstrap_ci(vals, seed=0)\n",
    "        means.append(m)\n",
    "        cis.append((m-lo, hi-m))\n",
    "    yerr = np.array(cis).T\n",
    "    plt.figure()\n",
    "    plt.bar(tags, means, yerr=yerr, capsize=4)\n",
    "    plt.ylabel(\"Test accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()\n",
    "    if outpath:\n",
    "        plt.savefig(outpath, dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97990392-1a40-4a78-a68a-04021becd3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "EXPERIMENT: C10_R18 | cifar10 | resnet18\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4544 val_acc=0.5490 test_acc=0.5672\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9358 val_acc=0.8762 test_acc=0.8845\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9925 val_acc=0.9064 test_acc=0.9208\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9996 val_acc=0.9214 test_acc=0.9323\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4541 val_acc=0.5418 test_acc=0.5644\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9354 val_acc=0.8704 test_acc=0.8746\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9916 val_acc=0.9134 test_acc=0.9187\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9996 val_acc=0.9240 test_acc=0.9283\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4558 val_acc=0.5374 test_acc=0.5637\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9343 val_acc=0.8662 test_acc=0.8769\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9917 val_acc=0.9086 test_acc=0.9164\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9997 val_acc=0.9254 test_acc=0.9291\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4802 val_acc=0.5638 test_acc=0.5816\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9380 val_acc=0.8776 test_acc=0.8912\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9930 val_acc=0.9126 test_acc=0.9229\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9997 val_acc=0.9218 test_acc=0.9309\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4796 val_acc=0.5674 test_acc=0.5855\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9350 val_acc=0.8692 test_acc=0.8801\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9916 val_acc=0.9148 test_acc=0.9225\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9996 val_acc=0.9262 test_acc=0.9339\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4788 val_acc=0.5722 test_acc=0.5829\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9348 val_acc=0.8704 test_acc=0.8756\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9916 val_acc=0.9142 test_acc=0.9225\n",
      "[ADAMW|cifar10|resnet18] ep=060 train_acc=0.9996 val_acc=0.9252 test_acc=0.9326\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.4934 val_acc=0.5726 test_acc=0.5864\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9415 val_acc=0.8724 test_acc=0.8833\n",
      "[ADAMW|cifar10|resnet18] ep=040 train_acc=0.9923 val_acc=0.9118 test_acc=0.9217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXPERIMENT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Tune AdamW + ALM(AdamW), and SGD + ALM(SGD)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m best_adamw \u001b[38;5;241m=\u001b[39m \u001b[43mtune_one_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msweep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m best_sgd   \u001b[38;5;241m=\u001b[39m tune_one_seed(ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m,   sweep, device)\n\u001b[1;32m     34\u001b[0m base_lr_wd_adamw, best_alm_adamw \u001b[38;5;241m=\u001b[39m tune_alm_one_seed(ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madamw\u001b[39m\u001b[38;5;124m\"\u001b[39m, sweep, device)\n",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m, in \u001b[0;36mtune_one_seed\u001b[0;34m(data_cfg, model_name, base_method, sweep, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m wd \u001b[38;5;129;01min\u001b[39;00m sweep\u001b[38;5;241m.\u001b[39madamw_wd:\n\u001b[1;32m     27\u001b[0m             cfg \u001b[38;5;241m=\u001b[39m TrainConfig(model\u001b[38;5;241m=\u001b[39mmodel_name, epochs\u001b[38;5;241m=\u001b[39mtune_epochs, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madamw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m             _, summ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m             candidates\u001b[38;5;241m.\u001b[39mappend((summ[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, wd\u001b[38;5;241m=\u001b[39mwd)))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m base_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 87\u001b[0m, in \u001b[0;36mtrain_one_run\u001b[0;34m(seed, data_cfg, cfg, device, log_every)\u001b[0m\n\u001b[1;32m     84\u001b[0m             aug_loss \u001b[38;5;241m=\u001b[39m base_loss \u001b[38;5;241m+\u001b[39m (lam \u001b[38;5;241m*\u001b[39m g_pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m rho \u001b[38;5;241m*\u001b[39m (g_pos \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(aug_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 87\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     89\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sweep = Sweep(seeds=[1,2,3,4,5], epochs=200)\n",
    "\n",
    "# ---- Define experiment suite ----\n",
    "suite = [\n",
    "    # Breadth\n",
    "    dict(name=\"C10_R18\",  data=DataConfig(dataset=\"cifar10\", batch_size=128, val_frac=0.1), model=\"resnet18\"),\n",
    "    dict(name=\"C10_R34\",  data=DataConfig(dataset=\"cifar10\", batch_size=128, val_frac=0.1), model=\"resnet34\"),\n",
    "    dict(name=\"C10_WRN\",  data=DataConfig(dataset=\"cifar10\", batch_size=128, val_frac=0.1), model=\"wrn28_10\"),\n",
    "    dict(name=\"C100_R18\", data=DataConfig(dataset=\"cifar100\", batch_size=128, val_frac=0.1), model=\"resnet18\"),\n",
    "    dict(name=\"C100_R34\", data=DataConfig(dataset=\"cifar100\", batch_size=128, val_frac=0.1), model=\"resnet34\"),\n",
    "\n",
    "    # Robustness: label noise\n",
    "    dict(name=\"C10_R18_noise20\", data=DataConfig(dataset=\"cifar10\", batch_size=128, val_frac=0.1, label_noise=0.2), model=\"resnet18\"),\n",
    "\n",
    "    # Robustness: stronger augmentation\n",
    "    dict(name=\"C10_R18_RA\", data=DataConfig(dataset=\"cifar10\", batch_size=128, val_frac=0.1, randaugment_N=2, randaugment_M=9), model=\"resnet18\"),\n",
    "\n",
    "    # Robustness: batch size shift (harder for some optimizers)\n",
    "    dict(name=\"C10_R18_bs512\", data=DataConfig(dataset=\"cifar10\", batch_size=512, val_frac=0.1), model=\"resnet18\"),\n",
    "]\n",
    "\n",
    "all_stats = []\n",
    "all_summaries = []\n",
    "all_histories = []\n",
    "\n",
    "for ex in suite:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"EXPERIMENT:\", ex[\"name\"], \"|\", ex[\"data\"].dataset, \"|\", ex[\"model\"])\n",
    "\n",
    "    # Tune AdamW + ALM(AdamW), and SGD + ALM(SGD)\n",
    "    best_adamw = tune_one_seed(ex[\"data\"], ex[\"model\"], \"adamw\", sweep, device)\n",
    "    best_sgd   = tune_one_seed(ex[\"data\"], ex[\"model\"], \"sgd\",   sweep, device)\n",
    "\n",
    "    base_lr_wd_adamw, best_alm_adamw = tune_alm_one_seed(ex[\"data\"], ex[\"model\"], \"adamw\", sweep, device)\n",
    "    base_lr_wd_sgd,   best_alm_sgd   = tune_alm_one_seed(ex[\"data\"], ex[\"model\"], \"sgd\",   sweep, device)\n",
    "\n",
    "    # Build final configs\n",
    "    cfg_adamw = TrainConfig(model=ex[\"model\"], method=\"adamw\", lr=best_adamw[\"lr\"], weight_decay=best_adamw[\"wd\"])\n",
    "    cfg_sgd   = TrainConfig(model=ex[\"model\"], method=\"sgd\",   lr=best_sgd[\"lr\"],   weight_decay=best_sgd[\"wd\"])\n",
    "\n",
    "    cfg_alm_adamw = TrainConfig(\n",
    "        model=ex[\"model\"], method=\"alm_adamw\",\n",
    "        lr=base_lr_wd_adamw[\"lr\"], weight_decay=base_lr_wd_adamw[\"wd\"],\n",
    "        alm=ALMConfig(budget_delta=best_alm_adamw[\"budget_delta\"], rho0=best_alm_adamw[\"rho0\"], dual_lr=best_alm_adamw[\"dual_lr\"])\n",
    "    )\n",
    "    cfg_alm_sgd = TrainConfig(\n",
    "        model=ex[\"model\"], method=\"alm_sgd\",\n",
    "        lr=base_lr_wd_sgd[\"lr\"], weight_decay=base_lr_wd_sgd[\"wd\"],\n",
    "        alm=ALMConfig(budget_delta=best_alm_sgd[\"budget_delta\"], rho0=best_alm_sgd[\"rho0\"], dual_lr=best_alm_sgd[\"dual_lr\"])\n",
    "    )\n",
    "\n",
    "    method_cfgs = {\n",
    "        \"AdamW\": cfg_adamw,\n",
    "        \"SGD\": cfg_sgd,\n",
    "        \"ALM-AdamW\": cfg_alm_adamw,\n",
    "        \"ALM-SGD\": cfg_alm_sgd,\n",
    "    }\n",
    "\n",
    "    hist_df, summ_df = final_multi_seed(ex[\"data\"], ex[\"model\"], method_cfgs, sweep, device)\n",
    "    summ_df[\"experiment\"] = ex[\"name\"]\n",
    "    hist_df[\"experiment\"] = ex[\"name\"]\n",
    "\n",
    "    all_summaries.append(summ_df)\n",
    "    all_histories.append(hist_df)\n",
    "\n",
    "    # Stats vs strong baselines\n",
    "    st1 = paired_stats(summ_df, \"AdamW\", \"ALM-AdamW\")\n",
    "    st2 = paired_stats(summ_df, \"SGD\",   \"ALM-SGD\")\n",
    "    st1[\"experiment\"] = ex[\"name\"]\n",
    "    st2[\"experiment\"] = ex[\"name\"]\n",
    "    all_stats += [st1, st2]\n",
    "\n",
    "    print(\"\\nPAIRED RESULTS:\", ex[\"name\"])\n",
    "    print(f\"AdamW mean={st1['mean_a']:.4f}  ALM-AdamW mean={st1['mean_b']:.4f}  diff={st1['mean_diff']:.4f}  p={st1['pval']:.6f}\")\n",
    "    print(f\"SGD   mean={st2['mean_a']:.4f}  ALM-SGD   mean={st2['mean_b']:.4f}  diff={st2['mean_diff']:.4f}  p={st2['pval']:.6f}\")\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    plot_final_bars(summ_df, title=f\"{ex['name']} final test acc (mean±CI)\", outpath=f\"figs/{ex['name']}_final.png\")\n",
    "\n",
    "final_summ = pd.concat(all_summaries, ignore_index=True)\n",
    "final_hist = pd.concat(all_histories, ignore_index=True)\n",
    "final_stats = pd.DataFrame(all_stats)\n",
    "\n",
    "display(final_stats)\n",
    "final_summ.to_csv(\"results_summary.csv\", index=False)\n",
    "final_stats.to_csv(\"results_stats.csv\", index=False)\n",
    "print(\"Saved: results_summary.csv, results_stats.csv, and figs/*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f2ae38-67f8-4bb4-b495-89acffbdc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int, deterministic: bool = False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # Note: full determinism on CUDA can require CUBLAS_WORKSPACE_CONFIG; we do not force it.\n",
    "\n",
    "def bootstrap_ci(xs: List[float], seed: int = 0, n: int = 4000, alpha: float = 0.05):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    xs = np.asarray(xs, dtype=np.float64)\n",
    "    if len(xs) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    means = []\n",
    "    for _ in range(n):\n",
    "        sample = rng.choice(xs, size=len(xs), replace=True)\n",
    "        means.append(sample.mean())\n",
    "    means = np.sort(means)\n",
    "    lo = np.quantile(means, alpha/2)\n",
    "    hi = np.quantile(means, 1-alpha/2)\n",
    "    return float(xs.mean()), float(lo), float(hi)\n",
    "\n",
    "def paired_permutation_test(a: List[float], b: List[float], seed: int = 0, n: int = 20000):\n",
    "    \"\"\"\n",
    "    H0: E[a-b]=0. Returns two-sided p-value.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a, dtype=np.float64)\n",
    "    b = np.asarray(b, dtype=np.float64)\n",
    "    d = a - b\n",
    "    obs = abs(d.mean())\n",
    "    cnt = 0\n",
    "    for _ in range(n):\n",
    "        signs = rng.choice([-1, 1], size=len(d))\n",
    "        perm = abs((d * signs).mean())\n",
    "        if perm >= obs:\n",
    "            cnt += 1\n",
    "    return (cnt + 1) / (n + 1)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857bb7fe-da77-4dda-b0f1-0bb2f1776c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
    "CIFAR100_STD  = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = \"cifar10\"    # \"cifar10\" or \"cifar100\"\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    val_frac: float = 0.1\n",
    "    data_dir: str = \"./data\"\n",
    "    split_seed: int = 123\n",
    "\n",
    "def make_loaders(cfg: DataConfig):\n",
    "    if cfg.dataset.lower() == \"cifar10\":\n",
    "        ds_cls = torchvision.datasets.CIFAR10\n",
    "        mean, std = CIFAR10_MEAN, CIFAR10_STD\n",
    "        num_classes = 10\n",
    "    elif cfg.dataset.lower() == \"cifar100\":\n",
    "        ds_cls = torchvision.datasets.CIFAR100\n",
    "        mean, std = CIFAR100_MEAN, CIFAR100_STD\n",
    "        num_classes = 100\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be cifar10 or cifar100\")\n",
    "\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    full_train = ds_cls(root=cfg.data_dir, train=True, download=True, transform=train_tf)\n",
    "    test_set   = ds_cls(root=cfg.data_dir, train=False, download=True, transform=test_tf)\n",
    "\n",
    "    n = len(full_train)\n",
    "    n_val = int(round(cfg.val_frac * n))\n",
    "    n_tr  = n - n_val\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(cfg.split_seed)\n",
    "    train_set, val_set = torch.utils.data.random_split(full_train, [n_tr, n_val], generator=g)\n",
    "\n",
    "    # IMPORTANT: val set should not use augmentation; swap transform\n",
    "    val_set.dataset = ds_cls(root=cfg.data_dir, train=True, download=True, transform=test_tf)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "        num_workers=cfg.num_workers, pin_memory=True, persistent_workers=(cfg.num_workers > 0)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "        num_workers=cfg.num_workers, pin_memory=True, persistent_workers=(cfg.num_workers > 0)\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "        num_workers=cfg.num_workers, pin_memory=True, persistent_workers=(cfg.num_workers > 0)\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028590e7-dd8e-432b-990a-904b5dfb157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cifar_resnet(depth: str, num_classes: int):\n",
    "    depth = depth.lower()\n",
    "    if depth == \"resnet18\":\n",
    "        m = torchvision.models.resnet18(num_classes=num_classes)\n",
    "    elif depth == \"resnet34\":\n",
    "        m = torchvision.models.resnet34(num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"depth must be resnet18 or resnet34\")\n",
    "\n",
    "    # CIFAR tweaks: 3x3 conv, stride1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    return m\n",
    "\n",
    "def make_scheduler(optimizer, epochs: int, steps_per_epoch: int, warmup_epochs: int = 5):\n",
    "    total_steps = epochs * steps_per_epoch\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a5a1e2-b653-4fa5-a831-4ceda8ae6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ALMConfig:\n",
    "    warmup_epochs: int = 5\n",
    "\n",
    "    # Budget calibration: B = logK_at_warmup_end + budget_delta\n",
    "    budget_delta: float = 0.50\n",
    "    margin: float = 0.0  # optional extra slack; budget_delta already gives feasibility\n",
    "\n",
    "    # Dual / penalty dynamics\n",
    "    lam0: float = 0.0\n",
    "    rho0: float = 0.1\n",
    "    dual_lr: float = 0.02\n",
    "\n",
    "    rho_growth: float = 1.6\n",
    "    rho_shrink: float = 0.9\n",
    "    rho_max: float = 2.0\n",
    "    rho_min: float = 0.05\n",
    "\n",
    "    lam_max: float = 3.0\n",
    "\n",
    "    # EMA on violation (stabilizes noisy stochastic constraint eval)\n",
    "    ema_beta: float = 0.95\n",
    "    tol: float = 1e-3\n",
    "    patience: int = 2\n",
    "\n",
    "    # Constraint evaluation frequency (steps)\n",
    "    constraint_every: int = 20\n",
    "\n",
    "    # Which layers are constrained: \"all\" or \"last\"\n",
    "    which: str = \"all\"\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    depth: str = \"resnet18\"\n",
    "    epochs: int = 100\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 0.02\n",
    "\n",
    "    method: str = \"adamw\"  # \"adamw\" or \"sgd\"\n",
    "    momentum: float = 0.9  # only for sgd\n",
    "\n",
    "    amp: bool = False\n",
    "    label_smoothing: float = 0.0\n",
    "\n",
    "    # ALM switch + parameters\n",
    "    use_alm: bool = False\n",
    "    alm: ALMConfig = field(default_factory=ALMConfig)\n",
    "\n",
    "    grad_clip: float = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bbf748-95fa-4134-8314-858f1220afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_constrained_weights(model: nn.Module, which: str = \"all\"):\n",
    "    \"\"\"\n",
    "    Yields (name, weight_tensor, kind) where kind in {\"conv\",\"linear\"}.\n",
    "    which=\"all\": all Conv2d + Linear weights\n",
    "    which=\"last\": only final fc weight\n",
    "    \"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and hasattr(m, \"weight\") and m.weight is not None:\n",
    "            if which == \"last\":\n",
    "                continue\n",
    "            yield f\"{name}.weight\", m.weight, \"conv\"\n",
    "        if isinstance(m, nn.Linear) and hasattr(m, \"weight\") and m.weight is not None:\n",
    "            if which == \"last\" and name != \"fc\":\n",
    "                continue\n",
    "            yield f\"{name}.weight\", m.weight, \"linear\"\n",
    "\n",
    "def power_iter_sigma(Wmat: torch.Tensor, u: torch.Tensor, iters: int = 1):\n",
    "    \"\"\"\n",
    "    One-step (or few-step) power iteration to estimate top singular value.\n",
    "    Returns (sigma, u_new, v_new). Uses no_grad updates for u/v stability.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iters):\n",
    "            v = torch.mv(Wmat.t(), u)\n",
    "            v = v / (v.norm() + 1e-12)\n",
    "            u = torch.mv(Wmat, v)\n",
    "            u = u / (u.norm() + 1e-12)\n",
    "        sigma = torch.dot(u, torch.mv(Wmat, v)).abs().clamp(min=1e-12)\n",
    "    return sigma, u, v\n",
    "\n",
    "class SpectralCache:\n",
    "    def __init__(self):\n",
    "        self.store: Dict[int, Dict[str, Any]] = {}\n",
    "        self.last_logK: float = float(\"nan\")\n",
    "        self.last_B: float = float(\"nan\")\n",
    "        self.last_sigmas: Dict[int, float] = {}\n",
    "\n",
    "def compute_logK_and_uv(model: nn.Module, cache: SpectralCache, which: str = \"all\", iters: int = 1):\n",
    "    \"\"\"\n",
    "    Computes logK = mean_i log sigma_i over constrained layers and returns:\n",
    "      - logK (float)\n",
    "      - per-layer u,v,sigma cached for gradient penalty injection\n",
    "    \"\"\"\n",
    "    sigmas = []\n",
    "    for _, W, kind in iter_constrained_weights(model, which=which):\n",
    "        key = id(W)\n",
    "        if key not in cache.store:\n",
    "            out_dim = W.shape[0]\n",
    "            u0 = torch.randn(out_dim, device=W.device, dtype=torch.float32)\n",
    "            u0 = u0 / (u0.norm() + 1e-12)\n",
    "            cache.store[key] = {\"u\": u0, \"v\": None, \"sigma\": None, \"kind\": kind, \"shape\": tuple(W.shape)}\n",
    "\n",
    "        entry = cache.store[key]\n",
    "        u = entry[\"u\"]\n",
    "\n",
    "        Wmat = W.reshape(W.shape[0], -1).detach()\n",
    "        sigma, u_new, v_new = power_iter_sigma(Wmat, u, iters=iters)\n",
    "\n",
    "        entry[\"u\"] = u_new\n",
    "        entry[\"v\"] = v_new\n",
    "        entry[\"sigma\"] = float(sigma.item())\n",
    "        cache.last_sigmas[key] = float(sigma.item())\n",
    "\n",
    "        sigmas.append(sigma)\n",
    "\n",
    "    if len(sigmas) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    sigmas = torch.stack(sigmas)\n",
    "    logK = torch.log(sigmas).mean().item()\n",
    "    cache.last_logK = float(logK)\n",
    "    return float(logK)\n",
    "\n",
    "def apply_alm_grad_penalty(model: nn.Module, cache: SpectralCache, g_pos: float, coef: float, which: str = \"all\"):\n",
    "    \"\"\"\n",
    "    Adds approx grad of (coef * g_pos) where g = logK - B, constraint logK <= B.\n",
    "    Using fixed u,v from latest power-iter: grad log sigma ≈ (1/sigma) * u v^T.\n",
    "    Global logK = mean log sigma => scale by 1/L.\n",
    "    \"\"\"\n",
    "    if g_pos <= 0 or coef <= 0:\n",
    "        return\n",
    "\n",
    "    # count constrained layers\n",
    "    keys = []\n",
    "    tensors = []\n",
    "    kinds = []\n",
    "    for _, W, kind in iter_constrained_weights(model, which=which):\n",
    "        key = id(W)\n",
    "        if key in cache.store and cache.store[key].get(\"v\", None) is not None:\n",
    "            keys.append(key); tensors.append(W); kinds.append(kind)\n",
    "    L = len(keys)\n",
    "    if L == 0:\n",
    "        return\n",
    "\n",
    "    for key, W, kind in zip(keys, tensors, kinds):\n",
    "        if W.grad is None:\n",
    "            continue\n",
    "        entry = cache.store[key]\n",
    "        sigma = float(entry[\"sigma\"])\n",
    "        u = entry[\"u\"].to(dtype=torch.float32)\n",
    "        v = entry[\"v\"].to(dtype=torch.float32)\n",
    "\n",
    "        # grad(log sigma) ≈ (1/sigma) * u v^T, scaled by 1/L\n",
    "        scale = (coef * (1.0 / max(sigma, 1e-12)) * (1.0 / L))\n",
    "        gmat = (u[:, None] * v[None, :]) * scale  # float32\n",
    "\n",
    "        gmat = gmat.reshape(W.shape[0], -1).reshape_as(W).to(dtype=W.grad.dtype)\n",
    "        W.grad.add_(gmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43b92ad5-ac58-489e-8b7a-710b52c168b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_run(seed: int, data_cfg: DataConfig, train_cfg: TrainConfig, device: str = \"cuda\", deterministic: bool = False, log_every: int = 10):\n",
    "    set_seed(seed, deterministic=deterministic)\n",
    "\n",
    "    train_loader, val_loader, test_loader, num_classes = make_loaders(data_cfg)\n",
    "    model = make_cifar_resnet(train_cfg.depth, num_classes=num_classes).to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=train_cfg.label_smoothing)\n",
    "\n",
    "    # optimizer\n",
    "    if train_cfg.method.lower() == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=train_cfg.lr, weight_decay=train_cfg.weight_decay)\n",
    "    elif train_cfg.method.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=train_cfg.lr, weight_decay=train_cfg.weight_decay,\n",
    "                              momentum=train_cfg.momentum, nesterov=True)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'adamw' or 'sgd'\")\n",
    "\n",
    "    scheduler = make_scheduler(optimizer, epochs=train_cfg.epochs, steps_per_epoch=len(train_loader), warmup_epochs=5)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=(train_cfg.amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    # ALM state\n",
    "    alm = train_cfg.alm\n",
    "    cache = SpectralCache()\n",
    "    B = None\n",
    "    lam = float(alm.lam0)\n",
    "    rho = float(alm.rho0)\n",
    "    g_ema = 0.0\n",
    "    bad_count = 0\n",
    "    good_count = 0\n",
    "\n",
    "    history = []\n",
    "    best_val = -1.0\n",
    "    test_at_best_val = -1.0\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    def eval_acc(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                total += y.numel()\n",
    "        return correct / max(1, total)\n",
    "\n",
    "    for ep in range(1, train_cfg.epochs + 1):\n",
    "        model.train()\n",
    "        tr_correct, tr_total = 0, 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            global_step += 1\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=(train_cfg.amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # IMPORTANT: unscale grads before adding ALM penalty + clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # ---- ALM budget calibration at end of warmup ----\n",
    "            if train_cfg.use_alm and ep == alm.warmup_epochs and B is None:\n",
    "                # compute logK on current weights\n",
    "                logK = compute_logK_and_uv(model, cache, which=alm.which, iters=1)\n",
    "                B = logK + float(alm.budget_delta) + float(alm.margin)\n",
    "                cache.last_B = float(B)\n",
    "\n",
    "            # ---- Constraint evaluation every N steps (cheap) ----\n",
    "            logK = float(\"nan\")\n",
    "            g_pos = 0.0\n",
    "            if train_cfg.use_alm and (B is not None) and (global_step % alm.constraint_every == 0):\n",
    "                logK = compute_logK_and_uv(model, cache, which=alm.which, iters=1)\n",
    "                g = float(logK - B)\n",
    "                g_pos = max(0.0, g)\n",
    "\n",
    "                # EMA of violation\n",
    "                g_ema = alm.ema_beta * g_ema + (1 - alm.ema_beta) * g_pos\n",
    "\n",
    "                # penalty coefficient for gradient term\n",
    "                coef = lam + rho * g_pos\n",
    "                apply_alm_grad_penalty(model, cache, g_pos=g_pos, coef=coef, which=alm.which)\n",
    "\n",
    "            # grad clip (helps stability; ICML reviewers expect stability story)\n",
    "            if train_cfg.grad_clip is not None and train_cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_cfg.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            tr_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            tr_total += y.numel()\n",
    "\n",
    "        # ---- epoch-end dual / rho update (stable, bounded) ----\n",
    "        if train_cfg.use_alm and (B is not None):\n",
    "            # dual ascent on positive constraint violation\n",
    "            lam = min(alm.lam_max, max(0.0, lam + alm.dual_lr * rho * g_ema))\n",
    "\n",
    "            # adaptive rho based on sustained violation / feasibility\n",
    "            if g_ema > alm.tol:\n",
    "                bad_count += 1\n",
    "                good_count = 0\n",
    "                if bad_count >= alm.patience:\n",
    "                    rho = min(alm.rho_max, rho * alm.rho_growth)\n",
    "                    bad_count = 0\n",
    "            elif g_ema < (alm.tol * 0.3):\n",
    "                good_count += 1\n",
    "                bad_count = 0\n",
    "                if good_count >= alm.patience:\n",
    "                    rho = max(alm.rho_min, rho * alm.rho_shrink)\n",
    "                    good_count = 0\n",
    "\n",
    "        train_acc = tr_correct / max(1, tr_total)\n",
    "        val_acc = eval_acc(val_loader)\n",
    "        test_acc = eval_acc(test_loader)\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            test_at_best_val = test_acc\n",
    "\n",
    "        row = {\n",
    "            \"seed\": seed,\n",
    "            \"method\": (\"ALM_\"+train_cfg.method.upper()) if train_cfg.use_alm else train_cfg.method.upper(),\n",
    "            \"epoch\": ep,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"best_val_acc_so_far\": best_val,\n",
    "            \"test_at_best_val_so_far\": test_at_best_val,\n",
    "            \"logK\": cache.last_logK,\n",
    "            \"B\": cache.last_B,\n",
    "            \"g_ema\": g_ema,\n",
    "            \"lam\": lam,\n",
    "            \"rho\": rho,\n",
    "        }\n",
    "        history.append(row)\n",
    "\n",
    "        if (ep == 1) or (ep % log_every == 0) or (ep == train_cfg.epochs):\n",
    "            tag = \"ALM\" if train_cfg.use_alm else train_cfg.method.upper()\n",
    "            extra = \"\"\n",
    "            if train_cfg.use_alm and (B is not None):\n",
    "                extra = f\" logK={cache.last_logK:.4f} g_ema={g_ema:.6f} lam={lam:.3f} rho={rho:.3f} B={cache.last_B:.4f}\"\n",
    "            print(f\"[{tag}|{data_cfg.dataset}|{train_cfg.depth}] ep={ep:03d} train_acc={train_acc:.4f} val_acc={val_acc:.4f} test_acc={test_acc:.4f}{extra}\")\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "\n",
    "    summary = {\n",
    "        \"seed\": seed,\n",
    "        \"method\": (\"ALM_\"+train_cfg.method.upper()) if train_cfg.use_alm else train_cfg.method.upper(),\n",
    "        \"final_test_acc\": float(hist_df.iloc[-1][\"test_acc\"]),\n",
    "        \"best_val_acc\": float(hist_df[\"val_acc\"].max()),\n",
    "        \"test_at_best_val\": float(hist_df.loc[hist_df[\"val_acc\"].idxmax(), \"test_acc\"]),\n",
    "    }\n",
    "    return hist_df, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de54b220-ce5c-4cd0-bf00-0560c106daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TinySweep:\n",
    "    # Keep this small.\n",
    "    lr_grid: Tuple[float, ...] = (3e-4, 5e-4)\n",
    "    wd_grid: Tuple[float, ...] = (0.02, 0.05)\n",
    "    # ALM knobs (small set)\n",
    "    budget_delta_grid: Tuple[float, ...] = (0.30, 0.50)\n",
    "    rho0_grid: Tuple[float, ...] = (0.05, 0.10)\n",
    "\n",
    "    seed_tune: int = 0\n",
    "    epochs_tune: int = 20\n",
    "\n",
    "    seeds_final: Tuple[int, ...] = (1,2,3)\n",
    "    epochs_final: int = 100\n",
    "\n",
    "def tiny_tune(data_cfg: DataConfig, depth: str, method: str, use_alm: bool, sweep: TinySweep, device: str):\n",
    "    \"\"\"\n",
    "    Returns best config by max val_acc over a cheap 20-epoch run.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_score = -1.0\n",
    "\n",
    "    for lr in sweep.lr_grid:\n",
    "        for wd in sweep.wd_grid:\n",
    "            if not use_alm:\n",
    "                tc = TrainConfig(depth=depth, epochs=sweep.epochs_tune, lr=lr, weight_decay=wd,\n",
    "                                 method=method, use_alm=False, amp=False)\n",
    "                df, summ = train_one_run(sweep.seed_tune, data_cfg, tc, device=device, deterministic=False, log_every=20)\n",
    "                score = float(df[\"val_acc\"].max())\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = {\"lr\": lr, \"wd\": wd}\n",
    "            else:\n",
    "                for bd in sweep.budget_delta_grid:\n",
    "                    for rho0 in sweep.rho0_grid:\n",
    "                        tc = TrainConfig(\n",
    "                            depth=depth, epochs=sweep.epochs_tune, lr=lr, weight_decay=wd,\n",
    "                            method=method, use_alm=True, amp=False,\n",
    "                            alm=ALMConfig(\n",
    "                                warmup_epochs=5,\n",
    "                                budget_delta=bd,\n",
    "                                rho0=rho0,\n",
    "                                dual_lr=0.02,\n",
    "                                rho_growth=1.6, rho_shrink=0.9,\n",
    "                                rho_max=2.0, lam_max=3.0,\n",
    "                                constraint_every=20,\n",
    "                                which=\"all\"\n",
    "                            )\n",
    "                        )\n",
    "                        df, summ = train_one_run(sweep.seed_tune, data_cfg, tc, device=device, deterministic=False, log_every=20)\n",
    "                        score = float(df[\"val_acc\"].max())\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best = {\"lr\": lr, \"wd\": wd, \"budget_delta\": bd, \"rho0\": rho0}\n",
    "\n",
    "    print(f\"[TUNE] best {('ALM_' if use_alm else '')}{method.upper()} score={best_score:.4f} cfg={best}\")\n",
    "    return best\n",
    "\n",
    "def final_compare(data_cfg: DataConfig, depth: str, sweep: TinySweep, tuned: Dict[str, Dict[str, Any]], device: str):\n",
    "    all_hist = []\n",
    "    all_summ = []\n",
    "\n",
    "    def run_cfg(seed, method, use_alm, cfg):\n",
    "        tc = TrainConfig(\n",
    "            depth=depth, epochs=sweep.epochs_final,\n",
    "            lr=float(cfg[\"lr\"]), weight_decay=float(cfg[\"wd\"]),\n",
    "            method=method, amp=False,\n",
    "            use_alm=use_alm\n",
    "        )\n",
    "        if use_alm:\n",
    "            tc.alm = ALMConfig(\n",
    "                warmup_epochs=5,\n",
    "                budget_delta=float(cfg[\"budget_delta\"]),\n",
    "                rho0=float(cfg[\"rho0\"]),\n",
    "                dual_lr=0.02,\n",
    "                rho_growth=1.6, rho_shrink=0.9,\n",
    "                rho_max=2.0, lam_max=3.0,\n",
    "                constraint_every=20,\n",
    "                which=\"all\"\n",
    "            )\n",
    "        df, summ = train_one_run(seed, data_cfg, tc, device=device, deterministic=False, log_every=20)\n",
    "        return df, summ\n",
    "\n",
    "    # Methods included (ICML baseline coverage, minimal compute)\n",
    "    methods = [\n",
    "        (\"sgd\",  False, \"SGD\"),\n",
    "        (\"adamw\",False, \"ADAMW\"),\n",
    "        (\"adamw\",True,  \"ALM_ADAMW\"),\n",
    "    ]\n",
    "\n",
    "    for seed in sweep.seeds_final:\n",
    "        for method, use_alm, tag in methods:\n",
    "            cfg = tuned[tag]\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(f\"RUN seed={seed} | {tag}\")\n",
    "            df, summ = run_cfg(seed, method, use_alm, cfg)\n",
    "            all_hist.append(df)\n",
    "            all_summ.append(summ)\n",
    "\n",
    "    summ_df = pd.DataFrame(all_summ)\n",
    "    hist_df = pd.concat(all_hist, ignore_index=True)\n",
    "\n",
    "    # Paired tests (same seeds): ALM vs AdamW, SGD vs AdamW, ALM vs SGD\n",
    "    def paired(methodA, methodB):\n",
    "        a = []\n",
    "        b = []\n",
    "        for s in sweep.seeds_final:\n",
    "            a.append(float(summ_df[(summ_df.seed==s) & (summ_df.method==methodA)][\"final_test_acc\"].iloc[0]))\n",
    "            b.append(float(summ_df[(summ_df.seed==s) & (summ_df.method==methodB)][\"final_test_acc\"].iloc[0]))\n",
    "        meanA, loA, hiA = bootstrap_ci(a, seed=0)\n",
    "        meanB, loB, hiB = bootstrap_ci(b, seed=1)\n",
    "        p = paired_permutation_test(a, b, seed=2)\n",
    "        diff = float(np.mean(np.array(a) - np.array(b)))\n",
    "        return (meanA, loA, hiA, meanB, loB, hiB, p, diff)\n",
    "\n",
    "    print(\"\\n==================== FINAL SUMMARY ====================\")\n",
    "    print(summ_df.sort_values([\"method\",\"seed\"]).reset_index(drop=True))\n",
    "\n",
    "    print(\"\\nPaired stats (final_test_acc):\")\n",
    "    mA, loA, hiA, mB, loB, hiB, p, diff = paired(\"ALM_ADAMW\", \"ADAMW\")\n",
    "    print(f\"ALM_ADAMW mean={mA:.4f} CI=[{loA:.4f},{hiA:.4f}] | ADAMW mean={mB:.4f} CI=[{loB:.4f},{hiB:.4f}] | p={p:.4f} | diff={diff:+.4f}\")\n",
    "\n",
    "    mA, loA, hiA, mB, loB, hiB, p, diff = paired(\"SGD\", \"ADAMW\")\n",
    "    print(f\"SGD      mean={mA:.4f} CI=[{loA:.4f},{hiA:.4f}] | ADAMW mean={mB:.4f} CI=[{loB:.4f},{hiB:.4f}] | p={p:.4f} | diff={diff:+.4f}\")\n",
    "\n",
    "    mA, loA, hiA, mB, loB, hiB, p, diff = paired(\"ALM_ADAMW\", \"SGD\")\n",
    "    print(f\"ALM_ADAMW mean={mA:.4f} CI=[{loA:.4f},{hiA:.4f}] | SGD   mean={mB:.4f} CI=[{loB:.4f},{hiB:.4f}] | p={p:.4f} | diff={diff:+.4f}\")\n",
    "\n",
    "    return summ_df, hist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181012a-f8bc-47c6-8e78-fa919b360b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[SGD|cifar10|resnet18] ep=001 train_acc=0.1061 val_acc=0.1128 test_acc=0.1097\n",
      "[SGD|cifar10|resnet18] ep=020 train_acc=0.3792 val_acc=0.3694 test_acc=0.3885\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[SGD|cifar10|resnet18] ep=001 train_acc=0.1061 val_acc=0.1124 test_acc=0.1099\n",
      "[SGD|cifar10|resnet18] ep=020 train_acc=0.3830 val_acc=0.3780 test_acc=0.3913\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[SGD|cifar10|resnet18] ep=001 train_acc=0.1080 val_acc=0.1256 test_acc=0.1253\n",
      "[SGD|cifar10|resnet18] ep=020 train_acc=0.4315 val_acc=0.4220 test_acc=0.4402\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[SGD|cifar10|resnet18] ep=001 train_acc=0.1082 val_acc=0.1258 test_acc=0.1251\n",
      "[SGD|cifar10|resnet18] ep=020 train_acc=0.4320 val_acc=0.4290 test_acc=0.4355\n",
      "[TUNE] best SGD score=0.4290 cfg={'lr': 0.0005, 'wd': 0.05}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.3037 val_acc=0.4286 test_acc=0.4417\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9550 val_acc=0.8926 test_acc=0.8908\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.3036 val_acc=0.4296 test_acc=0.4438\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9543 val_acc=0.8882 test_acc=0.8936\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.3361 val_acc=0.4484 test_acc=0.4592\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9684 val_acc=0.9006 test_acc=0.9054\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ADAMW|cifar10|resnet18] ep=001 train_acc=0.3354 val_acc=0.4444 test_acc=0.4520\n",
      "[ADAMW|cifar10|resnet18] ep=020 train_acc=0.9689 val_acc=0.9030 test_acc=0.9064\n",
      "[TUNE] best ADAMW score=0.9048 cfg={'lr': 0.0005, 'wd': 0.05}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3040 val_acc=0.4246 test_acc=0.4405\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9545 val_acc=0.8884 test_acc=0.8895 logK=0.8209 g_ema=0.201596 lam=0.027 rho=2.000 B=0.6192\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3039 val_acc=0.4280 test_acc=0.4402\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9550 val_acc=0.8902 test_acc=0.8923 logK=0.8095 g_ema=0.189407 lam=0.045 rho=2.000 B=0.6199\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3046 val_acc=0.4344 test_acc=0.4459\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9545 val_acc=0.8920 test_acc=0.8914 logK=0.8321 g_ema=0.012372 lam=0.000 rho=0.205 B=0.8195\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3039 val_acc=0.4268 test_acc=0.4411\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9535 val_acc=0.8896 test_acc=0.8916 logK=0.8354 g_ema=0.014888 lam=0.000 rho=0.430 B=0.8204\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3029 val_acc=0.4280 test_acc=0.4440\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9564 val_acc=0.8890 test_acc=0.8915 logK=0.7971 g_ema=0.182278 lam=0.024 rho=2.000 B=0.6150\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3034 val_acc=0.4306 test_acc=0.4444\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9546 val_acc=0.8916 test_acc=0.8911 logK=0.7895 g_ema=0.175178 lam=0.042 rho=2.000 B=0.6145\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3036 val_acc=0.4258 test_acc=0.4414\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9556 val_acc=0.8916 test_acc=0.8895 logK=0.8065 g_ema=0.000000 lam=0.000 rho=0.050 B=0.8161\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3042 val_acc=0.4266 test_acc=0.4429\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9542 val_acc=0.8910 test_acc=0.8921 logK=0.8081 g_ema=0.000000 lam=0.000 rho=0.050 B=0.8155\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3356 val_acc=0.4530 test_acc=0.4556\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9684 val_acc=0.9050 test_acc=0.9092 logK=1.0136 g_ema=0.378769 lam=0.051 rho=2.000 B=0.6352\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3365 val_acc=0.4460 test_acc=0.4529\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9675 val_acc=0.9056 test_acc=0.9096 logK=0.9937 g_ema=0.356423 lam=0.087 rho=2.000 B=0.6359\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3355 val_acc=0.4446 test_acc=0.4535\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9679 val_acc=0.9038 test_acc=0.9048 logK=1.0367 g_ema=0.201751 lam=0.016 rho=1.342 B=0.8351\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3362 val_acc=0.4486 test_acc=0.4550\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9672 val_acc=0.9026 test_acc=0.9092 logK=1.0306 g_ema=0.194977 lam=0.029 rho=2.000 B=0.8360\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3355 val_acc=0.4422 test_acc=0.4544\n",
      "[ALM|cifar10|resnet18] ep=020 train_acc=0.9692 val_acc=0.9018 test_acc=0.9051 logK=0.9829 g_ema=0.356355 lam=0.048 rho=2.000 B=0.6262\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[ALM|cifar10|resnet18] ep=001 train_acc=0.3367 val_acc=0.4492 test_acc=0.4571\n"
     ]
    }
   ],
   "source": [
    "# Choose ONE experiment first (cheap + decisive). If ALM wins, add cifar100/resnet18.\n",
    "data_cfg = DataConfig(dataset=\"cifar10\", batch_size=128, num_workers=4, val_frac=0.1, split_seed=123)\n",
    "\n",
    "depth = \"resnet18\"\n",
    "sweep = TinySweep(\n",
    "    lr_grid=(3e-4, 5e-4),\n",
    "    wd_grid=(0.02, 0.05),\n",
    "    budget_delta_grid=(0.30, 0.50),\n",
    "    rho0_grid=(0.05, 0.10),\n",
    "    seed_tune=0,\n",
    "    epochs_tune=20,\n",
    "    seeds_final=(1,2,3),\n",
    "    epochs_final=100\n",
    ")\n",
    "\n",
    "# Tiny tune each method (objective, cheap)\n",
    "tuned = {}\n",
    "tuned[\"SGD\"]      = tiny_tune(data_cfg, depth, method=\"sgd\",   use_alm=False, sweep=sweep, device=device)\n",
    "tuned[\"ADAMW\"]    = tiny_tune(data_cfg, depth, method=\"adamw\", use_alm=False, sweep=sweep, device=device)\n",
    "tuned[\"ALM_ADAMW\"]= tiny_tune(data_cfg, depth, method=\"adamw\", use_alm=True,  sweep=sweep, device=device)\n",
    "\n",
    "# Final 3-seed comparison\n",
    "summ_df, hist_df = final_compare(data_cfg, depth, sweep, tuned, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a20e0fb-9a1b-43e8-a2ae-5205f34e3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d333ae8-4301-4af8-80f4-f1821f25f5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
